{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TFM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMZUdQBumUcNDZvkK4TPNDg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martavillagran/martavillagran.github.io/blob/main/TFM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbe7etjox_im"
      },
      "source": [
        "# Pruebas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3hpVQVUIB1n"
      },
      "source": [
        "### **Prueba para comprobar el truco de la matriz identidad**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgZItwfEDZx8"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yDMU6kCDYpL",
        "outputId": "d3a89df3-a5a6-48a0-a7ed-4fe28149e7f1"
      },
      "source": [
        "# Datos del mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVs0tZMnDWHR"
      },
      "source": [
        "# Parameters\n",
        "max_layers = 7\n",
        "max_layers_2train = 3\n",
        "num_max_units = 54\n",
        "input_dim = 28"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_uxhqZktrlF"
      },
      "source": [
        "def build_model(max_layers):\n",
        "  # Comprobaciones de la identidad tipo resnet (fijo las tres primeras capas y el resto no las entreno)\n",
        "  inputs = keras.Input(shape=(input_dim, input_dim), name=\"digits\")\n",
        "\n",
        "  for layers in range(max_layers):\n",
        "    if layers == 0:\n",
        "      x = keras.layers.Flatten()(inputs)\n",
        "    else:\n",
        "      if layers > 3:  \n",
        "        x = keras.layers.Dense(num_max_units, trainable=False, kernel_initializer=tf.keras.initializers.Identity())(x)\n",
        "      else:\n",
        "        x = keras.layers.Dense(num_max_units, activation=\"sigmoid\")(x)\n",
        "\n",
        "  outputs = keras.layers.Dense(len(np.unique(train_labels)), activation=\"softmax\", name=\"classification\")(x)\n",
        "  return  keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "vs5o5_FRD1Ko",
        "outputId": "3c5ec3c0-fa9b-4de9-c12c-fabe44839347"
      },
      "source": [
        "model1 = build_model(7)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-32d15d7b164a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-ecbdeee82ffc>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(max_layers)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_max_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"classification\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0;32mreturn\u001b[0m  \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRV9OJCpD46w"
      },
      "source": [
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXOQFVR5nghy"
      },
      "source": [
        "model2 = build_model(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u04kj0s5D_EE"
      },
      "source": [
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfgW8Qw22xOD"
      },
      "source": [
        "model1.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01), \n",
        "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNtG0Pfl2zLM"
      },
      "source": [
        "np.random.seed(19)\n",
        "tf.random.set_seed(41)\n",
        "history = model1.fit(train_images, train_labels, epochs=100, batch_size=512, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vd6r5iHC3Bxf"
      },
      "source": [
        "model1.predict(test_images[0:1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Yy9LT5H2sy"
      },
      "source": [
        "# Planteamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEOWCHLixfOt"
      },
      "source": [
        "#### Con Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uJlpon8H4gn"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68LLfOXYIAo6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cef8de5-6039-4073-ae7c-5bdc6ae35817"
      },
      "source": [
        "# Datos del mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2xm5wveH8h5"
      },
      "source": [
        "# Parameters\n",
        "max_layers = 6\n",
        "max_layers_2train = 2\n",
        "num_max_units = 54\n",
        "input_dim = 28\n",
        "trigger_thr = 0.025"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDgkjORZISpC"
      },
      "source": [
        "def build_model(max_layers, max_layers2train):\n",
        "  # Comprobaciones de la identidad tipo resnet (fijo las tres primeras capas y el resto no las entreno)\n",
        "  inputs = keras.Input(shape=(input_dim**2,), name=\"digits\")\n",
        "\n",
        "  for layers in range(max_layers):\n",
        "    if layers == 0:\n",
        "      x = keras.layers.Dense(num_max_units, activation=\"relu\", name=\"layer%s\" %layers, kernel_constraint=FreezeSlice([1],np.s_[0]))(inputs)\n",
        "    else:\n",
        "      if layers > max_layers2train:  \n",
        "        x = keras.layers.Dense(num_max_units, trainable=False,  name=\"layer%s\" %layers, \n",
        "                               kernel_initializer=tf.keras.initializers.Identity())(x)\n",
        "      else:\n",
        "        x = keras.layers.Dense(num_max_units,  name=\"layer%s\" %layers, activation=\"relu\")(x)\n",
        "\n",
        "  outputs = keras.layers.Dense(len(np.unique(train_labels)), activation=\"softmax\", name=\"classification\")(x)\n",
        "  return  keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mVLYDeiIj3J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "8371401c-0002-4599-815e-0e15b3aeb6dc"
      },
      "source": [
        "model = build_model(max_layers, max_layers_2train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-6-05dd31fc2331>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_layers_2train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-5-182729a65b6c>\u001b[0m in \u001b[0;36mbuild_model\u001b[1;34m(max_layers, max_layers2train)\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mlayers\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlayers\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_max_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"layer%s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_constraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFreezeSlice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mlayers\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mmax_layers2train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'FreezeSlice' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bx4G81owMk4Q",
        "outputId": "67bb1eac-d5c3-4fd0-e083-702c0f8cdbaa"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "digits (InputLayer)          [(None, 784)]             0         \n",
            "_________________________________________________________________\n",
            "layer0 (Dense)               (None, 54)                42390     \n",
            "_________________________________________________________________\n",
            "layer1 (Dense)               (None, 54)                2970      \n",
            "_________________________________________________________________\n",
            "layer2 (Dense)               (None, 54)                2970      \n",
            "_________________________________________________________________\n",
            "layer3 (Dense)               (None, 54)                2970      \n",
            "_________________________________________________________________\n",
            "layer4 (Dense)               (None, 54)                2970      \n",
            "_________________________________________________________________\n",
            "layer5 (Dense)               (None, 54)                2970      \n",
            "_________________________________________________________________\n",
            "classification (Dense)       (None, 10)                550       \n",
            "=================================================================\n",
            "Total params: 57,790\n",
            "Trainable params: 48,880\n",
            "Non-trainable params: 8,910\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbd5yBCQIqHy"
      },
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5rM0sfLJ24m"
      },
      "source": [
        "# Prepare the training dataset\n",
        "batch_size = 512\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = np.reshape(x_train, (-1, 784))\n",
        "x_test = np.reshape(x_test, (-1, 784))\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAXmGg6xMDqy"
      },
      "source": [
        "# Prepare the metrics.\n",
        "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONcqygmHME64"
      },
      "source": [
        "# Instantiate an optimizer.\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
        "# Instantiate a loss function.\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjqe-lwlKnfV"
      },
      "source": [
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x, training=True)\n",
        "        loss_value = loss_fn(y, logits)\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    train_acc_metric.update_state(y, logits)\n",
        "    return loss_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1JQudrAQimG"
      },
      "source": [
        "trigger_thr = 0.25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bo0DiUCJRYle"
      },
      "source": [
        "def get_non_trainable_layers(model, max_layers):\n",
        "  non_trainable = np.zeros(max_layers)\n",
        "  for i in range(max_layers):\n",
        "    if model.get_layer(\"layer%s\" %i).trainable == 0:\n",
        "      non_trainable[i] = 1\n",
        "  return non_trainable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv-7xrahvpz6"
      },
      "source": [
        "non_trainabale_layers = get_non_trainable_layers(model, max_layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8wQaMWPwOIN",
        "outputId": "c71e1061-40df-407b-fc8b-fea9798b4741"
      },
      "source": [
        "# Trainable weigths para hacer la mascara y el dropout\n",
        "# No sirve lo anterior, el modelo no se compila, hacerlo con early stopping\n",
        "# https://github.com/keras-team/keras/issues/2880 - para darle freeze"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., 1., 1., 1.])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "jK-H0y0A1tf6",
        "outputId": "22b96861-1538-49e9-9b5f-0ff4621d30bd"
      },
      "source": [
        "w1 = model.get_layer('layer0').get_weigths()[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-802a5550fc0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'layer0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weigth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Dense' object has no attribute 'get_weigth'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztoxMQZNJnqo",
        "outputId": "a210c920-d7bf-43d2-d0c4-e92ff088251c"
      },
      "source": [
        "# Custom training loop\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # Iterate over the batches of the dataset.\n",
        "    loss_prev = loss_value\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        loss_value = train_step(x_batch_train, y_batch_train)\n",
        "    \n",
        "    if epoch == 0:\n",
        "      loss_prev = loss_value\n",
        "    if np.abs((loss_prev - loss_value)/loss_prev) > trigger_thr:\n",
        "      # GA optimization ex\n",
        "      hidden_layers = 4\n",
        "      non_trainabale_layers = get_non_trainable_layers(model, max_layers)\n",
        "      while (max_layers - np.sum(non_trainabale_layers)) < hidden_layers:\n",
        "        print('Trigger')\n",
        "        idx = np.where(non_trainabale_layers>0)[0]\n",
        "        non_trainabale_layers[idx[0]] = 0\n",
        "        model.get_layer(\"layer%s\" %idx[0]).trainable = True\n",
        "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy') # ojo que sino no hace nada\n",
        "        \n",
        "   \n",
        "    # Display metrics at the end of each epoch.\n",
        "    train_acc = train_acc_metric.result()\n",
        "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
        "     # Reset training metrics at the end of each epoch\n",
        "    train_acc_metric.reset_states()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training acc over epoch: 0.8779\n",
            "Training acc over epoch: 0.8813\n",
            "Training acc over epoch: 0.8869\n",
            "Training acc over epoch: 0.8920\n",
            "Training acc over epoch: 0.8947\n",
            "Training acc over epoch: 0.8969\n",
            "Training acc over epoch: 0.8993\n",
            "Training acc over epoch: 0.9020\n",
            "Training acc over epoch: 0.9039\n",
            "Training acc over epoch: 0.9062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u0iUp3yyPl8",
        "outputId": "f45114fa-5e0e-45b6-92c8-1cc52c1b6e94"
      },
      "source": [
        "non_trainabale_layers = get_non_trainable_layers(model, max_layers)\n",
        "non_trainabale_layers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 1., 1.])"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvLIGknvzRFR"
      },
      "source": [
        "w3 = model.get_layer('layer0')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv_hXYu4_pHQ"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "Ckn16TWc_Zzu",
        "outputId": "3a9ddc72-5064-4922-b10d-d2a5ba8fc1fd"
      },
      "source": [
        "pd.DataFrame(w3.get_weights()[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.040816</td>\n",
              "      <td>0.034933</td>\n",
              "      <td>0.071282</td>\n",
              "      <td>-0.005608</td>\n",
              "      <td>0.012143</td>\n",
              "      <td>0.036048</td>\n",
              "      <td>0.077783</td>\n",
              "      <td>0.037841</td>\n",
              "      <td>0.014904</td>\n",
              "      <td>0.082379</td>\n",
              "      <td>-0.000381</td>\n",
              "      <td>0.069258</td>\n",
              "      <td>-0.020570</td>\n",
              "      <td>0.000968</td>\n",
              "      <td>0.015307</td>\n",
              "      <td>0.064013</td>\n",
              "      <td>-0.077571</td>\n",
              "      <td>-0.006489</td>\n",
              "      <td>-0.019111</td>\n",
              "      <td>0.037761</td>\n",
              "      <td>-0.032844</td>\n",
              "      <td>-0.041544</td>\n",
              "      <td>-0.055568</td>\n",
              "      <td>-0.074368</td>\n",
              "      <td>0.023561</td>\n",
              "      <td>-0.081544</td>\n",
              "      <td>0.018014</td>\n",
              "      <td>0.064763</td>\n",
              "      <td>0.045914</td>\n",
              "      <td>0.038636</td>\n",
              "      <td>-0.079743</td>\n",
              "      <td>0.036667</td>\n",
              "      <td>0.015317</td>\n",
              "      <td>-0.021623</td>\n",
              "      <td>0.083662</td>\n",
              "      <td>0.055733</td>\n",
              "      <td>0.048132</td>\n",
              "      <td>-0.025575</td>\n",
              "      <td>-0.014567</td>\n",
              "      <td>0.038181</td>\n",
              "      <td>-0.047241</td>\n",
              "      <td>0.014633</td>\n",
              "      <td>0.082796</td>\n",
              "      <td>-0.013425</td>\n",
              "      <td>-0.050871</td>\n",
              "      <td>0.001069</td>\n",
              "      <td>-0.042327</td>\n",
              "      <td>-0.073835</td>\n",
              "      <td>0.035631</td>\n",
              "      <td>0.064500</td>\n",
              "      <td>0.065276</td>\n",
              "      <td>0.068856</td>\n",
              "      <td>-0.064112</td>\n",
              "      <td>0.065959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.050813</td>\n",
              "      <td>0.004208</td>\n",
              "      <td>-0.083369</td>\n",
              "      <td>-0.072771</td>\n",
              "      <td>0.040086</td>\n",
              "      <td>0.053143</td>\n",
              "      <td>0.023650</td>\n",
              "      <td>-0.017038</td>\n",
              "      <td>0.021519</td>\n",
              "      <td>-0.023179</td>\n",
              "      <td>-0.054561</td>\n",
              "      <td>0.043894</td>\n",
              "      <td>-0.082774</td>\n",
              "      <td>-0.002985</td>\n",
              "      <td>-0.027646</td>\n",
              "      <td>-0.010465</td>\n",
              "      <td>-0.024704</td>\n",
              "      <td>0.077425</td>\n",
              "      <td>0.069002</td>\n",
              "      <td>0.033896</td>\n",
              "      <td>0.068864</td>\n",
              "      <td>-0.002189</td>\n",
              "      <td>0.008090</td>\n",
              "      <td>0.043587</td>\n",
              "      <td>0.069004</td>\n",
              "      <td>-0.058721</td>\n",
              "      <td>-0.058498</td>\n",
              "      <td>0.056573</td>\n",
              "      <td>-0.022667</td>\n",
              "      <td>0.040349</td>\n",
              "      <td>-0.034662</td>\n",
              "      <td>-0.027642</td>\n",
              "      <td>-0.050791</td>\n",
              "      <td>-0.013329</td>\n",
              "      <td>-0.019086</td>\n",
              "      <td>-0.051741</td>\n",
              "      <td>0.036200</td>\n",
              "      <td>-0.003845</td>\n",
              "      <td>0.077286</td>\n",
              "      <td>0.038562</td>\n",
              "      <td>-0.074627</td>\n",
              "      <td>-0.074661</td>\n",
              "      <td>0.018292</td>\n",
              "      <td>-0.049099</td>\n",
              "      <td>-0.049046</td>\n",
              "      <td>-0.011546</td>\n",
              "      <td>-0.023001</td>\n",
              "      <td>-0.014542</td>\n",
              "      <td>0.014810</td>\n",
              "      <td>0.071854</td>\n",
              "      <td>0.028379</td>\n",
              "      <td>-0.014995</td>\n",
              "      <td>-0.033035</td>\n",
              "      <td>0.010024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.066511</td>\n",
              "      <td>-0.063230</td>\n",
              "      <td>0.060301</td>\n",
              "      <td>-0.022914</td>\n",
              "      <td>0.019500</td>\n",
              "      <td>-0.018194</td>\n",
              "      <td>-0.049797</td>\n",
              "      <td>-0.007767</td>\n",
              "      <td>0.052476</td>\n",
              "      <td>0.075698</td>\n",
              "      <td>0.052329</td>\n",
              "      <td>-0.028242</td>\n",
              "      <td>-0.084026</td>\n",
              "      <td>-0.026509</td>\n",
              "      <td>-0.030610</td>\n",
              "      <td>0.048289</td>\n",
              "      <td>0.025827</td>\n",
              "      <td>0.059023</td>\n",
              "      <td>0.062186</td>\n",
              "      <td>-0.055522</td>\n",
              "      <td>0.015328</td>\n",
              "      <td>0.023914</td>\n",
              "      <td>0.014982</td>\n",
              "      <td>0.060698</td>\n",
              "      <td>-0.036607</td>\n",
              "      <td>0.060766</td>\n",
              "      <td>0.068325</td>\n",
              "      <td>-0.036000</td>\n",
              "      <td>0.044746</td>\n",
              "      <td>-0.045154</td>\n",
              "      <td>0.051430</td>\n",
              "      <td>-0.055435</td>\n",
              "      <td>0.035434</td>\n",
              "      <td>0.073502</td>\n",
              "      <td>-0.079585</td>\n",
              "      <td>0.082807</td>\n",
              "      <td>0.022626</td>\n",
              "      <td>0.030297</td>\n",
              "      <td>0.021089</td>\n",
              "      <td>-0.081252</td>\n",
              "      <td>-0.030687</td>\n",
              "      <td>0.038162</td>\n",
              "      <td>0.018953</td>\n",
              "      <td>-0.056651</td>\n",
              "      <td>0.039238</td>\n",
              "      <td>-0.013250</td>\n",
              "      <td>0.053461</td>\n",
              "      <td>-0.057019</td>\n",
              "      <td>0.067539</td>\n",
              "      <td>-0.036395</td>\n",
              "      <td>0.029410</td>\n",
              "      <td>-0.082958</td>\n",
              "      <td>-0.016458</td>\n",
              "      <td>0.010619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.022178</td>\n",
              "      <td>-0.021026</td>\n",
              "      <td>-0.082399</td>\n",
              "      <td>-0.070535</td>\n",
              "      <td>0.019096</td>\n",
              "      <td>-0.005691</td>\n",
              "      <td>-0.053594</td>\n",
              "      <td>0.027403</td>\n",
              "      <td>0.039142</td>\n",
              "      <td>0.038024</td>\n",
              "      <td>-0.078203</td>\n",
              "      <td>0.079007</td>\n",
              "      <td>-0.074097</td>\n",
              "      <td>0.032382</td>\n",
              "      <td>-0.043261</td>\n",
              "      <td>-0.020954</td>\n",
              "      <td>-0.071257</td>\n",
              "      <td>-0.067479</td>\n",
              "      <td>0.023143</td>\n",
              "      <td>0.050282</td>\n",
              "      <td>-0.004035</td>\n",
              "      <td>0.065023</td>\n",
              "      <td>0.035802</td>\n",
              "      <td>0.066714</td>\n",
              "      <td>-0.018054</td>\n",
              "      <td>-0.006602</td>\n",
              "      <td>0.046304</td>\n",
              "      <td>0.020962</td>\n",
              "      <td>-0.020979</td>\n",
              "      <td>0.037289</td>\n",
              "      <td>-0.042101</td>\n",
              "      <td>-0.058693</td>\n",
              "      <td>-0.045718</td>\n",
              "      <td>-0.022028</td>\n",
              "      <td>-0.068745</td>\n",
              "      <td>-0.008206</td>\n",
              "      <td>-0.030656</td>\n",
              "      <td>-0.049251</td>\n",
              "      <td>-0.083461</td>\n",
              "      <td>0.029621</td>\n",
              "      <td>-0.008434</td>\n",
              "      <td>0.069664</td>\n",
              "      <td>-0.081643</td>\n",
              "      <td>-0.011234</td>\n",
              "      <td>-0.061039</td>\n",
              "      <td>-0.081420</td>\n",
              "      <td>-0.044808</td>\n",
              "      <td>-0.031347</td>\n",
              "      <td>-0.037301</td>\n",
              "      <td>-0.071862</td>\n",
              "      <td>-0.062580</td>\n",
              "      <td>0.053458</td>\n",
              "      <td>-0.049285</td>\n",
              "      <td>0.007102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.056877</td>\n",
              "      <td>-0.083174</td>\n",
              "      <td>-0.018631</td>\n",
              "      <td>0.037361</td>\n",
              "      <td>-0.028160</td>\n",
              "      <td>0.072845</td>\n",
              "      <td>0.069202</td>\n",
              "      <td>-0.026534</td>\n",
              "      <td>-0.017204</td>\n",
              "      <td>0.046108</td>\n",
              "      <td>0.002416</td>\n",
              "      <td>0.019517</td>\n",
              "      <td>-0.082398</td>\n",
              "      <td>-0.012950</td>\n",
              "      <td>0.024216</td>\n",
              "      <td>-0.051724</td>\n",
              "      <td>0.076221</td>\n",
              "      <td>-0.070663</td>\n",
              "      <td>-0.068563</td>\n",
              "      <td>0.067489</td>\n",
              "      <td>0.045596</td>\n",
              "      <td>-0.024708</td>\n",
              "      <td>0.044227</td>\n",
              "      <td>0.040457</td>\n",
              "      <td>-0.030351</td>\n",
              "      <td>0.072548</td>\n",
              "      <td>-0.011113</td>\n",
              "      <td>0.042733</td>\n",
              "      <td>0.047204</td>\n",
              "      <td>-0.004216</td>\n",
              "      <td>-0.005111</td>\n",
              "      <td>-0.055785</td>\n",
              "      <td>-0.044020</td>\n",
              "      <td>-0.040617</td>\n",
              "      <td>-0.072027</td>\n",
              "      <td>-0.007951</td>\n",
              "      <td>0.064382</td>\n",
              "      <td>-0.075824</td>\n",
              "      <td>-0.019558</td>\n",
              "      <td>-0.056610</td>\n",
              "      <td>0.002980</td>\n",
              "      <td>0.078838</td>\n",
              "      <td>0.082166</td>\n",
              "      <td>0.010410</td>\n",
              "      <td>-0.037415</td>\n",
              "      <td>-0.040607</td>\n",
              "      <td>0.035714</td>\n",
              "      <td>-0.067893</td>\n",
              "      <td>-0.074733</td>\n",
              "      <td>0.046354</td>\n",
              "      <td>-0.084028</td>\n",
              "      <td>0.052057</td>\n",
              "      <td>-0.039099</td>\n",
              "      <td>-0.079448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>779</th>\n",
              "      <td>0.032553</td>\n",
              "      <td>0.073165</td>\n",
              "      <td>-0.074960</td>\n",
              "      <td>0.035447</td>\n",
              "      <td>0.072694</td>\n",
              "      <td>-0.015046</td>\n",
              "      <td>0.050601</td>\n",
              "      <td>-0.050443</td>\n",
              "      <td>-0.058342</td>\n",
              "      <td>-0.025527</td>\n",
              "      <td>0.012865</td>\n",
              "      <td>-0.063225</td>\n",
              "      <td>0.042553</td>\n",
              "      <td>-0.006569</td>\n",
              "      <td>0.069413</td>\n",
              "      <td>0.065219</td>\n",
              "      <td>0.003376</td>\n",
              "      <td>-0.076669</td>\n",
              "      <td>-0.055099</td>\n",
              "      <td>-0.009938</td>\n",
              "      <td>-0.075990</td>\n",
              "      <td>-0.022195</td>\n",
              "      <td>-0.029973</td>\n",
              "      <td>0.053166</td>\n",
              "      <td>-0.073553</td>\n",
              "      <td>-0.019086</td>\n",
              "      <td>0.035778</td>\n",
              "      <td>-0.002784</td>\n",
              "      <td>0.068967</td>\n",
              "      <td>0.080992</td>\n",
              "      <td>-0.076994</td>\n",
              "      <td>0.000833</td>\n",
              "      <td>0.079333</td>\n",
              "      <td>0.004919</td>\n",
              "      <td>-0.076813</td>\n",
              "      <td>0.075796</td>\n",
              "      <td>-0.048707</td>\n",
              "      <td>0.016452</td>\n",
              "      <td>-0.018926</td>\n",
              "      <td>-0.068866</td>\n",
              "      <td>0.005920</td>\n",
              "      <td>-0.046256</td>\n",
              "      <td>0.021791</td>\n",
              "      <td>0.054079</td>\n",
              "      <td>0.019203</td>\n",
              "      <td>-0.028588</td>\n",
              "      <td>0.061442</td>\n",
              "      <td>-0.008230</td>\n",
              "      <td>-0.001006</td>\n",
              "      <td>-0.037633</td>\n",
              "      <td>-0.000933</td>\n",
              "      <td>0.059720</td>\n",
              "      <td>0.013516</td>\n",
              "      <td>-0.067722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>780</th>\n",
              "      <td>0.031789</td>\n",
              "      <td>0.061051</td>\n",
              "      <td>0.024838</td>\n",
              "      <td>-0.054059</td>\n",
              "      <td>0.083717</td>\n",
              "      <td>-0.020930</td>\n",
              "      <td>0.020352</td>\n",
              "      <td>0.000282</td>\n",
              "      <td>-0.052710</td>\n",
              "      <td>0.030490</td>\n",
              "      <td>-0.025399</td>\n",
              "      <td>-0.013316</td>\n",
              "      <td>-0.051764</td>\n",
              "      <td>0.056875</td>\n",
              "      <td>0.049697</td>\n",
              "      <td>-0.083116</td>\n",
              "      <td>-0.025027</td>\n",
              "      <td>-0.059617</td>\n",
              "      <td>-0.030312</td>\n",
              "      <td>-0.071241</td>\n",
              "      <td>0.075783</td>\n",
              "      <td>0.005492</td>\n",
              "      <td>0.027992</td>\n",
              "      <td>-0.053718</td>\n",
              "      <td>-0.052705</td>\n",
              "      <td>-0.080325</td>\n",
              "      <td>0.003682</td>\n",
              "      <td>0.016135</td>\n",
              "      <td>-0.038168</td>\n",
              "      <td>0.040071</td>\n",
              "      <td>0.082938</td>\n",
              "      <td>-0.057800</td>\n",
              "      <td>-0.009995</td>\n",
              "      <td>0.002813</td>\n",
              "      <td>0.079940</td>\n",
              "      <td>0.052179</td>\n",
              "      <td>0.007601</td>\n",
              "      <td>0.076367</td>\n",
              "      <td>0.012326</td>\n",
              "      <td>0.070553</td>\n",
              "      <td>0.047625</td>\n",
              "      <td>-0.077731</td>\n",
              "      <td>0.062256</td>\n",
              "      <td>0.001734</td>\n",
              "      <td>0.059669</td>\n",
              "      <td>0.055450</td>\n",
              "      <td>0.010006</td>\n",
              "      <td>-0.012430</td>\n",
              "      <td>0.063227</td>\n",
              "      <td>-0.067751</td>\n",
              "      <td>0.053841</td>\n",
              "      <td>0.035348</td>\n",
              "      <td>-0.045475</td>\n",
              "      <td>-0.078142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>781</th>\n",
              "      <td>-0.015362</td>\n",
              "      <td>-0.062178</td>\n",
              "      <td>-0.064879</td>\n",
              "      <td>0.013043</td>\n",
              "      <td>0.082827</td>\n",
              "      <td>0.026861</td>\n",
              "      <td>-0.020804</td>\n",
              "      <td>0.015974</td>\n",
              "      <td>-0.005130</td>\n",
              "      <td>0.020539</td>\n",
              "      <td>0.083357</td>\n",
              "      <td>-0.058618</td>\n",
              "      <td>-0.052433</td>\n",
              "      <td>-0.027116</td>\n",
              "      <td>-0.074733</td>\n",
              "      <td>-0.040232</td>\n",
              "      <td>0.016490</td>\n",
              "      <td>0.007047</td>\n",
              "      <td>0.052962</td>\n",
              "      <td>-0.064858</td>\n",
              "      <td>0.063549</td>\n",
              "      <td>0.080273</td>\n",
              "      <td>-0.009069</td>\n",
              "      <td>0.032298</td>\n",
              "      <td>-0.023046</td>\n",
              "      <td>-0.075692</td>\n",
              "      <td>-0.015669</td>\n",
              "      <td>0.047195</td>\n",
              "      <td>0.038616</td>\n",
              "      <td>-0.080842</td>\n",
              "      <td>0.047979</td>\n",
              "      <td>-0.035570</td>\n",
              "      <td>0.003754</td>\n",
              "      <td>-0.034984</td>\n",
              "      <td>0.013576</td>\n",
              "      <td>-0.032136</td>\n",
              "      <td>0.025569</td>\n",
              "      <td>0.040749</td>\n",
              "      <td>-0.006006</td>\n",
              "      <td>0.049025</td>\n",
              "      <td>-0.035446</td>\n",
              "      <td>0.048632</td>\n",
              "      <td>-0.006849</td>\n",
              "      <td>-0.007876</td>\n",
              "      <td>-0.071042</td>\n",
              "      <td>-0.018907</td>\n",
              "      <td>0.071958</td>\n",
              "      <td>-0.039329</td>\n",
              "      <td>-0.017456</td>\n",
              "      <td>0.029641</td>\n",
              "      <td>-0.071429</td>\n",
              "      <td>-0.040518</td>\n",
              "      <td>0.020316</td>\n",
              "      <td>0.027230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>782</th>\n",
              "      <td>-0.069466</td>\n",
              "      <td>-0.022379</td>\n",
              "      <td>-0.016960</td>\n",
              "      <td>0.041653</td>\n",
              "      <td>0.009581</td>\n",
              "      <td>-0.007190</td>\n",
              "      <td>-0.033757</td>\n",
              "      <td>0.051400</td>\n",
              "      <td>0.033781</td>\n",
              "      <td>-0.060344</td>\n",
              "      <td>-0.055742</td>\n",
              "      <td>-0.068006</td>\n",
              "      <td>0.049393</td>\n",
              "      <td>-0.075221</td>\n",
              "      <td>0.032562</td>\n",
              "      <td>0.013639</td>\n",
              "      <td>-0.054440</td>\n",
              "      <td>0.016070</td>\n",
              "      <td>-0.052705</td>\n",
              "      <td>-0.007346</td>\n",
              "      <td>-0.032531</td>\n",
              "      <td>-0.001831</td>\n",
              "      <td>0.025769</td>\n",
              "      <td>0.030416</td>\n",
              "      <td>0.007193</td>\n",
              "      <td>-0.023706</td>\n",
              "      <td>0.010714</td>\n",
              "      <td>0.053001</td>\n",
              "      <td>-0.060701</td>\n",
              "      <td>0.020528</td>\n",
              "      <td>-0.024327</td>\n",
              "      <td>0.049881</td>\n",
              "      <td>0.015018</td>\n",
              "      <td>-0.068961</td>\n",
              "      <td>0.078054</td>\n",
              "      <td>0.031498</td>\n",
              "      <td>0.004624</td>\n",
              "      <td>-0.027589</td>\n",
              "      <td>-0.047333</td>\n",
              "      <td>-0.066362</td>\n",
              "      <td>-0.046674</td>\n",
              "      <td>0.035396</td>\n",
              "      <td>-0.064848</td>\n",
              "      <td>0.044697</td>\n",
              "      <td>0.037344</td>\n",
              "      <td>0.001197</td>\n",
              "      <td>-0.054121</td>\n",
              "      <td>0.023626</td>\n",
              "      <td>0.040397</td>\n",
              "      <td>-0.015908</td>\n",
              "      <td>0.036753</td>\n",
              "      <td>0.012100</td>\n",
              "      <td>-0.008558</td>\n",
              "      <td>0.020920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>783</th>\n",
              "      <td>-0.013121</td>\n",
              "      <td>-0.040022</td>\n",
              "      <td>-0.011737</td>\n",
              "      <td>0.014154</td>\n",
              "      <td>0.059860</td>\n",
              "      <td>-0.012816</td>\n",
              "      <td>0.065015</td>\n",
              "      <td>0.063662</td>\n",
              "      <td>0.040852</td>\n",
              "      <td>-0.042863</td>\n",
              "      <td>0.014336</td>\n",
              "      <td>0.033264</td>\n",
              "      <td>0.071868</td>\n",
              "      <td>-0.061755</td>\n",
              "      <td>0.056502</td>\n",
              "      <td>0.072831</td>\n",
              "      <td>0.002276</td>\n",
              "      <td>-0.067977</td>\n",
              "      <td>-0.062773</td>\n",
              "      <td>0.066153</td>\n",
              "      <td>-0.082736</td>\n",
              "      <td>-0.031542</td>\n",
              "      <td>0.039183</td>\n",
              "      <td>-0.044154</td>\n",
              "      <td>0.084103</td>\n",
              "      <td>0.025029</td>\n",
              "      <td>0.019501</td>\n",
              "      <td>-0.024696</td>\n",
              "      <td>0.007616</td>\n",
              "      <td>-0.061201</td>\n",
              "      <td>-0.021905</td>\n",
              "      <td>-0.078541</td>\n",
              "      <td>-0.024903</td>\n",
              "      <td>0.044914</td>\n",
              "      <td>-0.031520</td>\n",
              "      <td>0.063232</td>\n",
              "      <td>-0.067512</td>\n",
              "      <td>-0.002836</td>\n",
              "      <td>-0.033181</td>\n",
              "      <td>0.030234</td>\n",
              "      <td>0.017389</td>\n",
              "      <td>-0.038101</td>\n",
              "      <td>0.053879</td>\n",
              "      <td>0.061957</td>\n",
              "      <td>-0.055167</td>\n",
              "      <td>0.051720</td>\n",
              "      <td>-0.015853</td>\n",
              "      <td>-0.069993</td>\n",
              "      <td>-0.034819</td>\n",
              "      <td>0.004436</td>\n",
              "      <td>-0.031047</td>\n",
              "      <td>-0.046359</td>\n",
              "      <td>0.002474</td>\n",
              "      <td>-0.077197</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>784 rows  54 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2   ...        51        52        53\n",
              "0   -0.040816  0.034933  0.071282  ...  0.068856 -0.064112  0.065959\n",
              "1    0.050813  0.004208 -0.083369  ... -0.014995 -0.033035  0.010024\n",
              "2    0.066511 -0.063230  0.060301  ... -0.082958 -0.016458  0.010619\n",
              "3    0.022178 -0.021026 -0.082399  ...  0.053458 -0.049285  0.007102\n",
              "4    0.056877 -0.083174 -0.018631  ...  0.052057 -0.039099 -0.079448\n",
              "..        ...       ...       ...  ...       ...       ...       ...\n",
              "779  0.032553  0.073165 -0.074960  ...  0.059720  0.013516 -0.067722\n",
              "780  0.031789  0.061051  0.024838  ...  0.035348 -0.045475 -0.078142\n",
              "781 -0.015362 -0.062178 -0.064879  ... -0.040518  0.020316  0.027230\n",
              "782 -0.069466 -0.022379 -0.016960  ...  0.012100 -0.008558  0.020920\n",
              "783 -0.013121 -0.040022 -0.011737  ... -0.046359  0.002474 -0.077197\n",
              "\n",
              "[784 rows x 54 columns]"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ4o02sXzYWg"
      },
      "source": [
        "from keras.initializers import Initializer\n",
        "\n",
        "class FixSlice(Initializer):\n",
        "    \"\"\"\n",
        "    Initializer which forces a certain slice to be chosen values\n",
        "\n",
        "    INPUTS:\n",
        "\n",
        "    values - An object which can be converted into a numpy ndarray. These are\n",
        "             the pre-chosen values. When using this initializer, the user should\n",
        "             ensure that the dtype of values can be converted to the desired\n",
        "             dtype of the weight tensor.\n",
        "\n",
        "    slice - A slice or tuple of slices (it is recommended to use numpy.s_ to\n",
        "            specify this parameter). This specifies which entries should be\n",
        "            filled with the pre-chosen values. When using this initializer,\n",
        "            the user should ensure that the slice object \"fits inside\" the shape\n",
        "            of the tensor to be initialized, and that the resulting slice of the\n",
        "            tensor has the same shape as the values ndarray.\n",
        "\n",
        "    backup - An initializer instance. The remaining values are filled using this\n",
        "             initializer.\n",
        "    \"\"\"\n",
        "    def __init__(self, values, slice, backup=\"glorot_uniform\"):\n",
        "        if hasattr(values, \"numpy\"):\n",
        "            self.values = values.numpy()\n",
        "        elif isinstance(values, np.ndarray):\n",
        "            self.values = values\n",
        "        else:\n",
        "            try:\n",
        "                self.values = values.to_numpy()\n",
        "            except:\n",
        "                self.values = np.array(values)\n",
        "\n",
        "        self.values = values\n",
        "        self.slice = slice\n",
        "        self.backup = initializers.get(backup)\n",
        "\n",
        "    def __call__(self, shape, dtype=None):\n",
        "        result = self.backup(shape, dtype=dtype).numpy()\n",
        "        result[self.slice] = self.values\n",
        "        return tf.Variable(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6wpBIE_9exn"
      },
      "source": [
        "from keras.constraints import Constraint\n",
        "\n",
        "class FreezeSlice(Constraint):\n",
        "    \"\"\"\n",
        "    Constraint which keeps a certain slice frozen at chosen values\n",
        "\n",
        "    INPUTS:\n",
        "\n",
        "    values - An object which can be converted into a numpy ndarray. These are\n",
        "             the pre-chosen values. When using this constraint, the user should\n",
        "             ensure that the dtype of values can be converted to the desired\n",
        "             dtype of the weight tensor.\n",
        "\n",
        "    slice - A slice or tuple of slices (it is recommended to use numpy.s_ to\n",
        "            specify this parameter). This specifies which entries should be\n",
        "            filled with the pre-chosen values. When using this initializer,\n",
        "            the user should ensure that the slice object \"fits inside\" the shape\n",
        "            of the tensor to be initialized, and that the resulting slice of the\n",
        "            tensor has the same shape as the values ndarray.\n",
        "    \"\"\"\n",
        "    def __init__(self, values, slice):\n",
        "        if hasattr(values, \"numpy\"):\n",
        "            self.values = values.numpy()\n",
        "        elif isinstance(values, np.ndarray):\n",
        "            self.values = values\n",
        "        else:\n",
        "            try:\n",
        "                self.values = values.to_numpy()\n",
        "            except:\n",
        "                self.values = np.array(values)\n",
        "\n",
        "        self.values = values\n",
        "        self.slice = slice\n",
        "\n",
        "    def __call__(self, w):\n",
        "        zs = np.zeros(w.shape)\n",
        "        zs[self.slice] = self.values\n",
        "        os = np.ones(w.shape)\n",
        "        os[self.slice] = 0\n",
        "        return w * os + zs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdRE_B5m-Gh_"
      },
      "source": [
        "model.add(Dense(1,input_dim =2,kernel_constraint=FreezeSlice([1],np.s_[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-fNC2S8xkyo"
      },
      "source": [
        "#### Con TensorFlow 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQXwypiTxm_r"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_q9SEJ6xwbk"
      },
      "source": [
        "# Datos del mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4dLknl9HoOE"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "ymwxMRf-HjNW",
        "outputId": "d3e5b0d7-6d2d-460e-d1c2-264afa9ce041"
      },
      "source": [
        "plt.imshow(train_images[0], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f58380f7c50>"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYE3q22cx0Vj"
      },
      "source": [
        "# Parameters\n",
        "max_layers = 2\n",
        "num_max_units = 54\n",
        "input_dim = 28\n",
        "output_dim = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gvz8DCj20z49"
      },
      "source": [
        "# Mask to freeze layers or neurons in each layer\n",
        "# Each neuron is trainable if 1, otherwise is weight = 0\n",
        "hidden_mask = tf.Variable(tf.cast(tf.random.normal((num_max_units, max_layers))>(0.5), dtype=tf.dtypes.float64))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEgaDi2i3QIg"
      },
      "source": [
        "def get_trainable_layers(hidden_mask):\n",
        "  # if one column is all filled by zeros, the layer is not trainable and its weitghs have to be equal to I(54)\n",
        "  return np.all(hidden_mask.numpy() == 0, axis=0) == False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fllA46cw-SgB"
      },
      "source": [
        "# Creacin de las variables para el modelo con un diccionario\n",
        "b = {}\n",
        "w = {}\n",
        "\n",
        "for layer in range(max_layers):\n",
        "  trainable_layers = get_trainable_layers(hidden_mask)\n",
        "  if layer == 0:\n",
        "    b['b%s' % (layer+1)] = tf.Variable(tf.random.normal(shape=[num_max_units, 1], dtype=tf.dtypes.float64))\n",
        "    w['w%s' % (layer+1)] =  tf.Variable(tf.random.normal(shape=[num_max_units, input_dim**2], dtype=tf.dtypes.float64))\n",
        "  else:\n",
        "    if trainable_layers[layer] == True:\n",
        "      b['b%s' % (layer+1)]  =  tf.Variable(tf.random.normal(shape=[num_max_units, 1], dtype=tf.dtypes.float64))\n",
        "      w['w%s' % (layer+1)]  =  tf.Variable(tf.random.normal(shape=[num_max_units, num_max_units], dtype=tf.dtypes.float64))\n",
        "    else:\n",
        "      b['b%s' % (layer+1)]  =  tf.Variable(tf.zeros(shape=[num_max_units, num_max_units], dtype=tf.dtypes.float64))\n",
        "      w['w%s' % (layer+1)]  =  tf.Variable(tf.eye(num_max_units))\n",
        "\n",
        "b['b%s' % (max_layers+1)]  =  tf.Variable(tf.random.normal(shape=[output_dim, 1], dtype=tf.dtypes.float64))\n",
        "w['w%s' % (max_layers+1)]  =  tf.Variable(tf.random.normal(shape=[output_dim, num_max_units], dtype=tf.dtypes.float64))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4owqgs-qAo27"
      },
      "source": [
        "# Parametros de la red neuronal\n",
        "hidden_activation = tf.nn.relu\n",
        "output_activation = tf.nn.softmax\n",
        "loss_fcn = tf.nn.sparse_softmax_cross_entropy_with_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWUQ2G5_58Ai"
      },
      "source": [
        "# Creacin de la red neuronal\n",
        "\n",
        "def get_loss(x, t, max_layers, hidden_mask):\n",
        "  a_prev = 0\n",
        "  trainable_layers = get_trainable_layers(hidden_mask)\n",
        "  for layer in range(max_layers):\n",
        "    w_aux = tf.transpose(tf.transpose(w['w%s' % (layer+1)])*hidden_mask[:,layer])\n",
        "    b_aux = tf.transpose(tf.transpose(b['b%s' % (layer+1)])*hidden_mask[:,layer])\n",
        "    if layer == 0: # For the input layer\n",
        "      a = hidden_activation(tf.matmul(w_aux, x.T) + b_aux)\n",
        "    else: # For the hidden layers\n",
        "      if trainable_layers[layer] == 0:\n",
        "        w_aux = w['w%s' % (layer+1)]\n",
        "        b_aux = b['b%s' % (layer+1)]\n",
        "      a = hidden_activation(tf.matmul(w_aux, a_prev) + b_aux)\n",
        "    a_prev = a\n",
        "  # For the ouput layer\n",
        "  a = tf.matmul(w['w%s' % (max_layers+1)], a_prev) + b['b%s' % (max_layers+1)]\n",
        "  y = output_activation(a, axis=0)\n",
        "\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVonw8Zp62VC"
      },
      "source": [
        "# Variables \n",
        "\n",
        "x_train = np.reshape(train_images, (train_images.shape[0], input_dim**2))\n",
        "x_test = np.reshape(test_images, (test_images.shape[0], input_dim**2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESC-cO-YIr_t"
      },
      "source": [
        "# Custom training loop from scratch (Parameters)\n",
        "lr = 1e-1\n",
        "trigger_thr = 0.025\n",
        "nepochs = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YLZy-SFKPiJ"
      },
      "source": [
        "def train(lr, trigger_thr, nepochs, x, t, max_layers):\n",
        "  training_loss = []\n",
        "  for epoch in range(nepochs):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "      # Compute the forward pass\n",
        "      y = get_loss(x, t, max_layers, hidden_mask)\n",
        "      xentropy = loss_fcn(tf.cast(tf.reshape(t, [-1]), dtype='int32'), tf.transpose(y))\n",
        "      loss = tf.reduce_mean(xentropy)\n",
        "      \n",
        "\n",
        "    training_loss.append(loss.numpy())\n",
        "\n",
        "    trainable_layers = get_trainable_layers(hidden_mask)\n",
        "\n",
        "    for layer in range(max_layers):\n",
        "      if trainable_layers[layer] == 1:\n",
        "        w_aux = tf.transpose(tf.transpose(w['w%s' % (layer+1)])*hidden_mask[:,layer])\n",
        "        b_aux = tf.transpose(tf.transpose(b['b%s' % (layer+1)])*hidden_mask[:,layer])\n",
        "        # Compute gradients \n",
        "        db = tape.gradient(loss, b_aux)\n",
        "        dw = tape.gradient(loss, w_aux)\n",
        "        # Update parameters\n",
        "        # w['w%s' % (layer+1)] = w['w%s' % (layer+1)] - lr*dw\n",
        "        # b['b%s' % (layer+1)] = b['b%s' % (layer+1)] - lr*db\n",
        "        w['w%s' % (layer+1)].assign(w['w%s' % (layer+1)] - lr*dw)\n",
        "        b['b%s' % (layer+1)].assign(b['b%s' % (layer+1)] - lr*db)\n",
        "  return training_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHvFAw-Yl3KK"
      },
      "source": [
        "training_loss = []\n",
        "for epoch in range(nepochs):\n",
        "  with tf.GradientTape(persistent=True) as tape:\n",
        "    # Compute the forward pass\n",
        "    y = get_loss(x_train, train_labels, max_layers, hidden_mask)\n",
        "    xentropy = loss_fcn(tf.cast(tf.reshape(train_labels, [-1]), dtype='int32'), tf.transpose(y))\n",
        "    loss = tf.reduce_mean(xentropy)\n",
        "    layer = 0\n",
        "    w_aux = tf.Variable(tf.transpose(tf.transpose(w['w%s' % (layer+1)])*hidden_mask[:,layer]))\n",
        "    b_aux = tf.Variable(tf.transpose(tf.transpose(b['b%s' % (layer+1)])*hidden_mask[:,layer]))\n",
        "  # Compute gradients \n",
        "  db = tape.gradient(loss, b_aux)\n",
        "  dw = tape.gradient(loss, w_aux)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93fIpoWOpHny"
      },
      "source": [
        "db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "AUxrCgrqXwmx",
        "outputId": "63a4f42c-43c6-4095-f44c-27befd66476b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualizacin \n",
        "plt.plot(training_loss)\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"poca\")\n",
        "plt.ylabel(\"coste\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpCWCS7HMEMr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl9uJGaLUlOW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM5a0vGjUlgS"
      },
      "source": [
        "#### Con TensorFlow 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZE603UQUlgT"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "___CSrNaUlgT"
      },
      "source": [
        "# Datos del mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B58C7IWJUlgU"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "FRdY6NiFUlgV",
        "outputId": "d3e5b0d7-6d2d-460e-d1c2-264afa9ce041"
      },
      "source": [
        "plt.imshow(train_images[0], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f58380f7c50>"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1L-UU1oUlgV"
      },
      "source": [
        "# Parameters\n",
        "max_layers = 2\n",
        "num_max_units = 54\n",
        "input_dim = 28\n",
        "output_dim = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzEZPHUoUlgW"
      },
      "source": [
        "# Mask to freeze layers or neurons in each layer\n",
        "# Each neuron is trainable if 1, otherwise is weight = 0\n",
        "hidden_mask = np.random.rand(num_max_units, max_layers)>(0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Hr2PM0mUlgW"
      },
      "source": [
        "def get_trainable_layers(hidden_mask):\n",
        "  # if one column is all filled by zeros, the layer is not trainable and its weitghs have to be equal to I(54)\n",
        "  return np.all(hidden_mask == 0, axis=0) == False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-DepMIyUlgX"
      },
      "source": [
        "# Creacin de las variables para el modelo con un diccionario\n",
        "b = {}\n",
        "w = {}\n",
        "\n",
        "for layer in range(max_layers):\n",
        "  # trainable_layers = get_trainable_layers(hidden_mask)\n",
        "  if layer == 0:\n",
        "    b['b%s' % (layer+1)] = tf.Variable(tf.random.normal(shape=[1, num_max_units], dtype=tf.dtypes.float64))\n",
        "    w['w%s' % (layer+1)] =  tf.Variable(tf.random.normal(shape=[input_dim**2, num_max_units], dtype=tf.dtypes.float64))\n",
        "  else:\n",
        "    # if trainable_layers[layer] == True:\n",
        "    b['b%s' % (layer+1)]  =  tf.Variable(tf.random.normal(shape=[1, num_max_units], dtype=tf.dtypes.float64))\n",
        "    w['w%s' % (layer+1)]  =  tf.Variable(tf.random.normal(shape=[num_max_units, num_max_units], dtype=tf.dtypes.float64))\n",
        "    # else:\n",
        "    #  b['b%s' % (layer+1)]  =  tf.Variable(tf.zeros(shape=[num_max_units, num_max_units], dtype=tf.dtypes.float64))\n",
        "    #  w['w%s' % (layer+1)]  =  tf.Variable(tf.eye(num_max_units))\n",
        "\n",
        "b['b%s' % (max_layers+1)]  =  tf.Variable(tf.random.normal(shape=[1, output_dim], dtype=tf.dtypes.float64))\n",
        "w['w%s' % (max_layers+1)]  =  tf.Variable(tf.random.normal(shape=[num_max_units, output_dim], dtype=tf.dtypes.float64))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oKnGaGsUlgX"
      },
      "source": [
        "# Parametros de la red neuronal\n",
        "hidden_activation = tf.nn.relu\n",
        "output_activation = tf.nn.softmax\n",
        "loss_fcn = tf.nn.sparse_softmax_cross_entropy_with_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qryz-fdJHTz",
        "outputId": "4a39d558-ec54-47e3-d135-e05679f5fa28"
      },
      "source": [
        "w['w%s' % (1)].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([784, 54])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RykkjAUqJayn"
      },
      "source": [
        "b_aux = b['b%s' % (1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGljey0RJMcs",
        "outputId": "bbe56161-c03d-4b82-8da2-f34d5dd2c9ed"
      },
      "source": [
        "b_aux.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([1, 54])"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgCbAmXuUlgX"
      },
      "source": [
        "# Creacin de la red neuronal\n",
        "\n",
        "def get_loss(x, t, max_layers):\n",
        "  a_prev = 0\n",
        "  # trainable_layers = get_trainable_layers(hidden_mask)\n",
        "  for layer in range(max_layers):\n",
        "    w_aux = w['w%s' % (layer+1)]\n",
        "    b_aux = b['b%s' % (layer+1)]\n",
        "    if layer == 0: # For the input layer\n",
        "      a = hidden_activation(tf.matmul(x, w_aux) + b_aux)\n",
        "    else: # For the hidden layers\n",
        "      # if trainable_layers[layer] == 0:\n",
        "      a = hidden_activation(tf.matmul(a_prev, w_aux) + b_aux)\n",
        "    a_prev = a\n",
        "  # For the ouput layer\n",
        "  a = tf.matmul(a_prev, w['w%s' % (max_layers+1)]) + b['b%s' % (max_layers+1)]\n",
        "  y = output_activation(a, axis=0)\n",
        "\n",
        "  return y, a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOg6MFceGW8P"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6pKxndXUlgY"
      },
      "source": [
        "# Variables \n",
        "\n",
        "x_train = tf.cast(np.reshape(train_images, (train_images.shape[0], input_dim**2)), dtype=tf.float64)\n",
        "x_test = np.reshape(test_images, (test_images.shape[0], input_dim**2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzUzTo43UlgY"
      },
      "source": [
        "# Custom training loop from scratch (Parameters)\n",
        "lr = 1e-3\n",
        "trigger_thr = 0.025\n",
        "nepochs = 2000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXQsa4QPUlgZ"
      },
      "source": [
        "def train(lr, trigger_thr, nepochs, x, t, max_layers):\n",
        "  training_loss = []\n",
        "  for epoch in range(nepochs):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "      # Compute the forward pass\n",
        "      y, a = get_loss(x, t, max_layers)\n",
        "      xentropy = loss_fcn(tf.cast(t, dtype='int32'), tf.transpose(y))\n",
        "      loss = tf.reduce_mean(xentropy)\n",
        "      \n",
        "\n",
        "    training_loss.append(loss.numpy())\n",
        "\n",
        "    # trainable_layers = get_trainable_layers(hidden_mask)\n",
        "\n",
        "    for layer in range(max_layers):\n",
        "      # if trainable_layers[layer] == 1:\n",
        "      w_aux = w['w%s' % (layer+1)]\n",
        "      b_aux = b['b%s' % (layer+1)]\n",
        "      # Compute gradients \n",
        "      db = tape.gradient(loss, b_aux)\n",
        "      dw = tape.gradient(loss, w_aux)\n",
        "      # Update parameters\n",
        "      w['w%s' % (layer+1)].assign(w['w%s' % (layer+1)] - lr*dw)\n",
        "      b['b%s' % (layer+1)].assign(b['b%s' % (layer+1)] - lr*db)\n",
        "  return training_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "rLxK3nONUlga",
        "outputId": "d64c3034-0800-4e92-a248-d25267ca5929"
      },
      "source": [
        "training_loss =  train(lr, trigger_thr, 20, x_train, train_labels, max_layers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-380644f17e50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_loss\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrigger_thr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-0bdd5a1dd377>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(lr, trigger_thr, nepochs, x, t, max_layers)\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0;31m# Compute the forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m       \u001b[0mxentropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxentropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msparse_softmax_cross_entropy_with_logits_v2\u001b[0;34m(labels, logits, name)\u001b[0m\n\u001b[1;32m   4350\u001b[0m   \"\"\"\n\u001b[1;32m   4351\u001b[0m   return sparse_softmax_cross_entropy_with_logits(\n\u001b[0;32m-> 4352\u001b[0;31m       labels=labels, logits=logits, name=name)\n\u001b[0m\u001b[1;32m   4353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msparse_softmax_cross_entropy_with_logits\u001b[0;34m(_sentinel, labels, logits, name)\u001b[0m\n\u001b[1;32m   4257\u001b[0m                        \u001b[0;34m\"should equal the shape of logits except for the last \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4258\u001b[0m                        \"dimension (received %s).\" % (labels_static_shape,\n\u001b[0;32m-> 4259\u001b[0;31m                                                      logits.get_shape()))\n\u001b[0m\u001b[1;32m   4260\u001b[0m     \u001b[0;31m# Check if no reshapes are required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4261\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Shape mismatch: The shape of labels (received (60000,)) should equal the shape of logits except for the last dimension (received (10, 60000))."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "P8WDopXKUlgb",
        "outputId": "323cfd68-97b6-46f2-a4ee-f74385eece37"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualizacin \n",
        "plt.plot(training_loss)\n",
        "plt.grid(True)\n",
        "plt.xlabel(\"poca\")\n",
        "plt.ylabel(\"coste\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bVFSPfyUlgb",
        "outputId": "0e2358e9-c26f-42ef-b852-e1f287ac2e77"
      },
      "source": [
        "len(training_loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvUw4w-iXzDa",
        "outputId": "c369c856-aa75-44e9-ca4e-a238a445e699"
      },
      "source": [
        "training_loss[20-1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.3678305804598163"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWPe2oXODec6"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEpMfA79EIXF"
      },
      "source": [
        "from tensorflow.keras.layers import Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKBVKCI7YHHR"
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "\n",
        "model.add(Dense(input_shape=(784, ), units=54, activation='relu'))\n",
        "model.add(Dense(units=54, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGHrdR3cCo8G",
        "outputId": "b9fb296a-bd70-4dad-cf8b-428148900190"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 54)                42390     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 54)                2970      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                550       \n",
            "=================================================================\n",
            "Total params: 45,910\n",
            "Trainable params: 45,910\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRfTzJljF8on"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-55S03UYFrA0"
      },
      "source": [
        "opt = tf.keras.optimizers.RMSprop(learning_rate=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRDyYAzDCo5k"
      },
      "source": [
        "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbxMve0zGHof",
        "outputId": "6ec817ed-9074-44cb-c48d-b64a87ba719b"
      },
      "source": [
        "model.fit(x_train, train_labels, epochs=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 1.3764\n",
            "Epoch 2/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.4330\n",
            "Epoch 3/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3615\n",
            "Epoch 4/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3421\n",
            "Epoch 5/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3215\n",
            "Epoch 6/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3062\n",
            "Epoch 7/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3042\n",
            "Epoch 8/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2966\n",
            "Epoch 9/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2880\n",
            "Epoch 10/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2721\n",
            "Epoch 11/20\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2881\n",
            "Epoch 12/20\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2671\n",
            "Epoch 13/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2660\n",
            "Epoch 14/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2713\n",
            "Epoch 15/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2576\n",
            "Epoch 16/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2598\n",
            "Epoch 17/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2521\n",
            "Epoch 18/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2358\n",
            "Epoch 19/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2398\n",
            "Epoch 20/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2470\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x7f2b35dff7a0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 546, in __del__\n",
            "    handle=self._handle, deleter=self._deleter)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1264, in delete_iterator\n",
            "    _ctx, \"DeleteIterator\", name, handle, deleter)\n",
            "KeyboardInterrupt: \n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2b2008b350>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2Qww2yAOigk"
      },
      "source": [
        "### Opcin con Kernels "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuY0B7AhGOfG"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtH0A8U-OtBB"
      },
      "source": [
        "def load_data():\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "    x_train = np.reshape(x_train, (x_train.shape[0], 784))/255.\n",
        "    x_test = np.reshape(x_test, (x_test.shape[0], 784))/255.\n",
        "    y_train = tf.keras.utils.to_categorical(y_train)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test)\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8Isic7NOwsK"
      },
      "source": [
        "def plot_results(history):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    epochs = len(history['val_loss'])\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(epochs), history['val_loss'], label='Val Loss')\n",
        "    plt.plot(range(epochs), history['train_loss'], label='Train Loss')\n",
        "    plt.xticks(list(range(epochs)))\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(epochs), history['val_acc'], label='Val Acc')\n",
        "    plt.xticks(list(range(epochs)))\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    return plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UniLY_CDUyVV"
      },
      "source": [
        "# Parameters\n",
        "max_layers = 2\n",
        "num_max_units = 128\n",
        "\n",
        "layers = np.zeros(max_layers, dtype='uint32')\n",
        "for i in range(max_layers): layers[i] = num_max_units"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA4KXWHtUbev"
      },
      "source": [
        "# Mask to freeze layers or neurons in each layer\n",
        "# Each neuron is trainable if 1, otherwise its weight = 0\n",
        "hidden_mask = tf.Variable(tf.cast(tf.random.normal((num_max_units, max_layers))>(-1), dtype=tf.dtypes.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPS6Gg0c1NJf"
      },
      "source": [
        "hidden_mask = tf.Variable(tf.cast(tf.ones((num_max_units, max_layers)), dtype=tf.dtypes.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1GTsd_JO7NB"
      },
      "source": [
        "class FluidNetwork:\n",
        "#------------------------------------------------------------------------------- \n",
        "    def __init__(self, layers, hidden_mask, input_dim, output_dim):\n",
        "        self.layers = layers\n",
        "        self.L = len(layers)\n",
        "        self.num_features = input_dim\n",
        "        self.num_classes = output_dim\n",
        "        self.hidden_mask = hidden_mask\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        self.W = {}\n",
        "        self.b = {}\n",
        "        \n",
        "        self.dW = {}\n",
        "        self.db = {}\n",
        "        \n",
        "        self.build_net()\n",
        "#-------------------------------------------------------------------------------     \n",
        "    def build_net(self):\n",
        "      trainable_layers = self.get_trainable_layers()\n",
        "      for i in range(self.L):\n",
        "        if i == 0:\n",
        "          self.W[i] = tf.Variable(tf.random.normal(shape=(self.layers[i], input_dim)))\n",
        "          self.b[i] = tf.Variable(tf.random.normal(shape=(self.layers[i], 1)))\n",
        "        else:\n",
        "          if trainable_layers[i] == 1:\n",
        "            self.W[i] = tf.Variable(tf.random.normal(shape=(self.layers[i], self.layers[i-1])))\n",
        "            self.b[i] = tf.Variable(tf.random.normal(shape=(self.layers[i], 1))) \n",
        "          else: \n",
        "            self.W[i] = tf.Variable(tf.eye(self.layers[i], self.layers[i-1]))\n",
        "            self.b[i] = tf.Variable(tf.zeros(shape=(self.layers[i], 1)))\n",
        "      self.W[i+1] = tf.Variable(tf.random.normal(shape=(output_dim, self.layers[i])))\n",
        "      self.b[i+1] = tf.Variable(tf.random.normal(shape=(output_dim, 1))) \n",
        "#-------------------------------------------------------------------------------             \n",
        "    def get_trainable_layers(self):\n",
        "    # if one column is all filled by zeros, the layer is not trainable and its weitghs have to be equal to I(units)\n",
        "      aux = np.all(self.hidden_mask.numpy() == 0, axis=0) == False\n",
        "      # aux[self.L-1] = 1 # output is always trainable\n",
        "      return aux\n",
        "#-------------------------------------------------------------------------------\n",
        "    def forward_pass(self, X):\n",
        "        A = tf.convert_to_tensor(X, dtype=tf.float32)\n",
        "\n",
        "        for i in range(self.L+1):\n",
        "            Z = tf.matmul(A, tf.transpose(self.W[i])) + tf.transpose(self.b[i])\n",
        "            if i <= self.L:\n",
        "                A = tf.nn.relu(Z)\n",
        "            else:\n",
        "                A = Z # do not call tf.nn.softmax with the output of a softmax as it internally does it\n",
        "        return A\n",
        "#-------------------------------------------------------------------------------\n",
        "    def compute_loss(self, A, Y):\n",
        "      loss = tf.nn.softmax_cross_entropy_with_logits(Y, A)\n",
        "      return tf.reduce_mean(loss)\n",
        "#-------------------------------------------------------------------------------    \n",
        "    def update_params(self, lr):\n",
        "        for i in range(self.L+1):\n",
        "          # if i == 0:\n",
        "          self.W[i].assign_sub(lr * self.dW[i])\n",
        "          self.b[i].assign_sub(lr * self.db[i])\n",
        "          # else:\n",
        "            # self.W[i].assign_sub(lr * self.hidden_mask[:, i-1:i] * self.dW[i])\n",
        "            # self.b[i].assign_sub(lr * self.db[i] * self.hidden_mask[:, i-1:i])\n",
        "#-------------------------------------------------------------------------------\n",
        "    def predict(self, X):\n",
        "      A = self.forward_pass(X)\n",
        "      return tf.argmax(tf.nn.softmax(A), axis=1)\n",
        "#-------------------------------------------------------------------------------    \n",
        "    def freeze_weights(self):\n",
        "      for i in range(1, self.L): # the first element in hidden layer is never used\n",
        "        self.W[i].assign = self.hidden_mask[:, i][None] * self.W[i]\n",
        "        self.b[i].assign = self.hidden_mask[:, i][None] * self.b[i]    \n",
        "#-------------------------------------------------------------------------------\n",
        "    def summary(self): # Actualizar\n",
        "        num_params = 0\n",
        "        for i in range(0, self.L+1):\n",
        "            num_params += self.W[i].shape[0] * self.W[i].shape[1]\n",
        "            num_params += self.b[i].shape[0]\n",
        "        print('Input Features:', self.num_features)\n",
        "        print('Number of Classes:', self.num_classes)\n",
        "        print('Hidden Layers:')\n",
        "        print('--------------')\n",
        "        for i in range(self.L):\n",
        "            print('Layer {}, Units {}'.format(i, self.layers[i]))\n",
        "        print('--------------')\n",
        "        print('Number of parameters:', num_params)\n",
        "#-------------------------------------------------------------------------------\n",
        "    def train_on_batch(self, X, Y, lr):\n",
        "      X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
        "      Y = tf.convert_to_tensor(Y, dtype=tf.float32)\n",
        "        \n",
        "      with tf.GradientTape(persistent=True) as tape:\n",
        "          self.freeze_weights()\n",
        "          A = self.forward_pass(X)\n",
        "          loss = self.compute_loss(A, Y)\n",
        "\n",
        "      for i in range(self.L):\n",
        "          self.dW[i] = tape.gradient(loss, self.W[i])\n",
        "          self.db[i] = tape.gradient(loss, self.b[i])\n",
        "\n",
        "      self.dW[i+1] = tape.gradient(loss, self.W[i+1])\n",
        "      self.db[i+1] = tape.gradient(loss, self.b[i+1])\n",
        "\n",
        "      del tape\n",
        "      self.update_params(lr)\n",
        "      return loss.numpy()\n",
        "#-------------------------------------------------------------------------------\n",
        "    def train(self, x_train, y_train, x_test, y_test, epochs, steps_per_epoch, batch_size, lr):\n",
        "      history = {\n",
        "          'val_loss':[],\n",
        "          'train_loss':[],\n",
        "          'val_acc':[]\n",
        "      }\n",
        "      \n",
        "      for e in range(0, epochs):\n",
        "          epoch_train_loss = 0.\n",
        "          print('Epoch{}'.format(e), end='-')\n",
        "          for i in range(0, steps_per_epoch):\n",
        "              x_batch = x_train[i*batch_size:(i+1)*batch_size]\n",
        "              y_batch = y_train[i*batch_size:(i+1)*batch_size]\n",
        "              \n",
        "              batch_loss = self.train_on_batch(x_batch, y_batch,lr)\n",
        "              epoch_train_loss += batch_loss\n",
        "              \n",
        "              if i%int(steps_per_epoch/10) == 0:\n",
        "                  print(end='.')\n",
        "                  \n",
        "          history['train_loss'].append(epoch_train_loss/steps_per_epoch)\n",
        "          val_A = self.forward_pass(x_test)\n",
        "          val_loss = self.compute_loss(val_A, y_test).numpy()\n",
        "          history['val_loss'].append(val_loss)\n",
        "          val_preds = self.predict(x_test)\n",
        "          val_acc =  np.mean(np.argmax(y_test, axis=1) == val_preds.numpy())\n",
        "          history['val_acc'].append(val_acc)\n",
        "          print('Val acc:',val_acc)\n",
        "      return history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoyO9A3pPnt7"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5TBTBu0P55f",
        "outputId": "ea90215b-1a4c-4f8f-818c-9465d277412a"
      },
      "source": [
        "input_dim = 28*28\n",
        "output_dim = 10\n",
        "net = FluidNetwork(layers, hidden_mask, input_dim, output_dim)\n",
        "net.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Features: 784\n",
            "Number of Classes: 10\n",
            "Hidden Layers:\n",
            "--------------\n",
            "Layer 0, Units 128\n",
            "Layer 1, Units 128\n",
            "--------------\n",
            "Number of parameters: 118282\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMi1oIu9P-fC"
      },
      "source": [
        "# Parameters\n",
        "batch_size = 104\n",
        "epochs = 100\n",
        "steps_per_epoch = int(x_train.shape[0]/batch_size)\n",
        "lr = 3e-3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm9e5Iu3zXSm"
      },
      "source": [
        "A = net.forward_pass(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0pLPCWB0ZF2"
      },
      "source": [
        "net.compute_loss(A, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUnMtzrKYSEZ"
      },
      "source": [
        "net.db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a28UavwQA6C"
      },
      "source": [
        "history = net.train(\n",
        "    x_train,y_train,\n",
        "    x_test, y_test,\n",
        "    epochs, steps_per_epoch,\n",
        "    batch_size, lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzNmZ4kuaKNj"
      },
      "source": [
        "hidden_mask = tf.Variable(tf.cast(tf.ones((num_max_units, max_layers)), dtype=tf.dtypes.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT1aINzjdi_w"
      },
      "source": [
        "net.b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FtZhQVznHPt",
        "outputId": "89f6a700-cd53-44d3-fbfb-ea350b3585fe"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEOjqhLmqbgL"
      },
      "source": [
        "### Opcion con padding en los pesos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3s9dC2UqjJj"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from operator import itemgetter\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import ShuffleSplit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwYjOiIX6JeG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa72cc00-ad1c-4516-aca1-8eef369096aa"
      },
      "source": [
        "X , (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfjajvDwqjJk"
      },
      "source": [
        "def load_data():\n",
        "  \"Loads data and each time the function is called a new partition of xtrain is used\"\n",
        "  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "  # Shuffle data\n",
        "  # rs = ShuffleSplit(n_splits=5, test_size=.1)\n",
        "  # x_train, y_train  = rs.get_n_splits(X) pensar \n",
        "  # Normalize and transform to categorical\n",
        "  x_train = np.reshape(x_train, (x_train.shape[0], 784))/255.\n",
        "  x_test = np.reshape(x_test, (x_test.shape[0], 784))/255.\n",
        "  y_train = tf.keras.utils.to_categorical(y_train)\n",
        "  y_test = tf.keras.utils.to_categorical(y_test)\n",
        "\n",
        "  return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUOFYl7p3WME"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKH0VhUcqjJk"
      },
      "source": [
        "def plot_results(history):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    epochs = len(history['val_loss'])\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(epochs), history['val_loss'], label='Val Loss')\n",
        "    plt.plot(range(epochs), history['train_loss'], label='Train Loss')\n",
        "    plt.xticks(list(range(epochs)))\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(epochs), history['val_acc'], label='Val Acc')\n",
        "    plt.xticks(list(range(epochs)))\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    return plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3AKV6olYj-K"
      },
      "source": [
        "class FluidNetwork:\n",
        "#-------------------------------------------------------------------------------    \n",
        "  def __init__(self, layers):\n",
        "      self.layers = layers\n",
        "      self.L = len(layers) # input, hidden & output layer\n",
        "      self.num_features = layers[0]\n",
        "      self.num_classes = layers[-1]\n",
        "      \n",
        "      self.W = {}\n",
        "      self.b = {}\n",
        "      \n",
        "      self.dW = {}\n",
        "      self.db = {}\n",
        "      \n",
        "      self.setup()\n",
        "      self.new_topology = []\n",
        "#-------------------------------------------------------------------------------\n",
        "  def setup(self):\n",
        "      \n",
        "      for i in range(1, self.L):\n",
        "        self.W[i] = tf.Variable(1.0, shape=tf.TensorShape(None)) # to enable future modifications\n",
        "        self.b[i] = tf.Variable(1.0, shape=tf.TensorShape(None)) \n",
        "        self.W[i].assign(tf.Variable(tf.random.normal(shape=(self.layers[i],self.layers[i-1]))))\n",
        "        self.b[i].assign(tf.Variable(tf.random.normal(shape=(self.layers[i],1))))\n",
        "#-------------------------------------------------------------------------------\n",
        "  def forward_pass(self, X):\n",
        "\n",
        "      A = tf.convert_to_tensor(X, dtype=tf.float32)\n",
        "      for i in range(1, self.L):\n",
        "          Z = tf.matmul(A,tf.transpose(self.W[i])) + tf.transpose(self.b[i])\n",
        "          if i != self.L-1:\n",
        "              A = tf.nn.relu(Z)\n",
        "          else:\n",
        "              A = Z\n",
        "      return A\n",
        "#-------------------------------------------------------------------------------\n",
        "  def compute_loss(self, A, Y):\n",
        "      loss = tf.nn.softmax_cross_entropy_with_logits(Y,A)\n",
        "      return tf.reduce_mean(loss)\n",
        "#-------------------------------------------------------------------------------   \n",
        "  def update_params(self, lr):\n",
        "      for i in range(1,self.L):\n",
        "          self.W[i].assign_sub(lr * self.dW[i])\n",
        "          self.b[i].assign_sub(lr * self.db[i])\n",
        "#-------------------------------------------------------------------------------          \n",
        "  def predict(self, X):\n",
        "\n",
        "      A = self.forward_pass(X)\n",
        "      return tf.argmax(tf.nn.softmax(A), axis=1)\n",
        "#-------------------------------------------------------------------------------  \n",
        "  def info(self):\n",
        "      num_params = 0\n",
        "      for i in range(1, self.L):\n",
        "          num_params += self.W[i].shape[0] * self.W[i].shape[1]\n",
        "          num_params += self.b[i].shape[0]\n",
        "      print('Input Features:', self.num_features)\n",
        "      print('Number of Classes:', self.num_classes)\n",
        "      print('Hidden Layers:')\n",
        "      print('--------------')\n",
        "      for i in range(1, self.L-1):\n",
        "          print('Layer {}, Units {}'.format(i, self.layers[i]))\n",
        "      print('--------------')\n",
        "      print('Number of parameters:', num_params)\n",
        "#-------------------------------------------------------------------------------\n",
        "  def train_on_batch(self, X, Y, lr):\n",
        "        \n",
        "      X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
        "      Y = tf.convert_to_tensor(Y, dtype=tf.float32)\n",
        "        \n",
        "      with tf.GradientTape(persistent=True) as tape:\n",
        "          A = self.forward_pass(X)\n",
        "          loss = self.compute_loss(A, Y)\n",
        "      for i in range(1, self.L):\n",
        "          self.dW[i] = tape.gradient(loss, self.W[i])\n",
        "          self.db[i] = tape.gradient(loss, self.b[i])\n",
        "      del tape\n",
        "      self.update_params(lr)\n",
        "      return loss.numpy()\n",
        "#-------------------------------------------------------------------------------    \n",
        "  def update_keys(self):\n",
        "    ini_list = {}\n",
        "    for i in range(1, net.L): ini_list[i] = i\n",
        "    self.b = dict(zip(ini_list, list(self.b.values())))    \n",
        "    self.W = dict(zip(ini_list, list(self.W.values())))   \n",
        "#-------------------------------------------------------------------------------\n",
        "  def activate_neurons(self, i):\n",
        "    \"Activate one neuron means adding a extra cols for W[i] and extra rows for W[i-1]\"\n",
        "    if self.layers[i-1] < self.new_topology[i-1]: # activate more neurons\n",
        "      neurons_to_act = self.new_topology[i-1] - self.layers[i-1]\n",
        "      if i != 2 :  # Only if is not the first layer of this case\n",
        "        # First add cols to W[i]\n",
        "        # W\n",
        "        aux = tf.Variable(tf.random.normal(shape=(self.new_topology[i], neurons_to_act)))\n",
        "        self.W[i].assign(tf.concat(axis=1, values=[self.W[i], aux]))\n",
        "      # Then add rows to W[i-1]\n",
        "      # W\n",
        "      aux = tf.Variable(tf.random.normal(shape=(neurons_to_act, self.new_topology[i-2])))\n",
        "      self.W[i-1].assign(tf.concat(axis=0, values=[self.W[i-1], aux]))\n",
        "      # b\n",
        "      aux = tf.Variable(tf.random.normal(shape=(neurons_to_act, 1)))\n",
        "      self.b[i-1].assign(tf.concat(axis=0, values=[self.b[i-1], aux]))\n",
        "    elif self.layers[i-1] > self.new_topology[i-1]: # deactivate some randomly neurons\n",
        "      neurons_to_act = self.new_topology[i-1]\n",
        "      random_index = np.random.choice(self.layers[i-1], neurons_to_act, replace=False)\n",
        "      if (i != self.L - 1 & self.L < len(self.new_topology)) | self.L >= len(self.new_topology): # Only if is not the last layer\n",
        "        # First remove extra cols from the current layer\n",
        "        # W\n",
        "        aux = pd.DataFrame(self.W[i].numpy())\n",
        "        aux = aux.reindex(columns=random_index).dropna(axis=1)\n",
        "        self.W[i].assign(aux.to_numpy())\n",
        "      # Then remove extra rows from the previous layer\n",
        "      # W\n",
        "      aux = np.transpose(pd.DataFrame(self.W[i-1].numpy()))\n",
        "      aux = aux.reindex(columns=random_index).dropna(axis=1)\n",
        "      self.W[i-1].assign(aux.T.to_numpy())\n",
        "      # b\n",
        "      aux = np.transpose(pd.DataFrame(self.b[i-1].numpy()))\n",
        "      aux = aux.reindex(columns=random_index).dropna(axis=1)\n",
        "      self.b[i-1].assign(aux.T.to_numpy())\n",
        "#-------------------------------------------------------------------------------    \n",
        "  def AG_update(self): # weight padding\n",
        "    prev_W = self.W.copy()\n",
        "    prev_b = self.b.copy()\n",
        "    new_L = len(self.new_topology)\n",
        "\n",
        "    for i in range(2, new_L): # 2 as the input alwyas remain the same\n",
        "      if self.L < new_L : # The new topology contains more layers\n",
        "        if i <= self.L - 1 : # Only apply for hidden layers\n",
        "          self.activate_neurons(i)\n",
        "        else: # add new layers,and reestructure output weights for the last layer\n",
        "          if i == new_L - 1: # copy the output weights from the previous structure\n",
        "            self.W[i] = tf.Variable(1.0, shape=tf.TensorShape(None)) # to enable future modifications\n",
        "            self.b[i] = tf.Variable(1.0, shape=tf.TensorShape(None)) \n",
        "            if (self.layers[-2] >= self.new_topology[-2]):\n",
        "              neurons_to_act = self.new_topology[i-1]\n",
        "              random_index = np.random.choice(self.layers[self.L-2], neurons_to_act, replace=False)\n",
        "              aux = pd.DataFrame(prev_W[self.L -1].numpy())\n",
        "              aux = aux.reindex(columns=random_index).dropna(axis=1)\n",
        "              self.W[i].assign(aux.to_numpy())\n",
        "            else:\n",
        "              neurons_to_act = self.new_topology[-2] - self.layers[-2]\n",
        "              aux = tf.Variable(tf.random.normal(shape=(self.new_topology[i], neurons_to_act)))\n",
        "              self.W[i].assign(tf.concat(axis=1, values=[prev_W[self.L -1], aux]))\n",
        "\n",
        "            self.b[i].assign(prev_b[self.L -1])\n",
        "          # Add more layers\n",
        "          self.W[i-1] = tf.Variable(1.0, shape=tf.TensorShape(None)) # to enable future modifications\n",
        "          self.b[i-1] = tf.Variable(1.0, shape=tf.TensorShape(None)) \n",
        "          self.W[i-1].assign(tf.Variable(tf.random.normal(shape=(self.new_topology[i-1],self.new_topology[i-2]))))\n",
        "          self.b[i-1].assign(tf.Variable(tf.random.normal(shape=(self.new_topology[i-1],1))))\n",
        "\n",
        "      elif self.L > new_L: # The new topology contains less layers\n",
        "        # Only apply for hidden layers\n",
        "        self.activate_neurons(i)\n",
        "        if i == new_L - 1 :\n",
        "          # First remove extra layers and then assign the ouput weights to the output layer\n",
        "          index_aux = np.arange(start=1, stop=new_L-1, step=1)\n",
        "          # index_aux[-1] = self.L - 1\n",
        "          mask = np.isin(list(self.W.keys()), index_aux) == False\n",
        "          keys_to_remove = np.array(list(self.W.keys()))[mask]\n",
        "          # W\n",
        "          d = self.W\n",
        "          l = keys_to_remove\n",
        "          list(map(d.__delitem__, filter(d.__contains__,l)))\n",
        "          # b\n",
        "          d = self.b\n",
        "          l = keys_to_remove\n",
        "          list(map(d.__delitem__, filter(d.__contains__,l)))\n",
        "          # Assign output weights to the ouput layer\n",
        "          self.W[i] = tf.Variable(1.0, shape=tf.TensorShape(None)) # to enable future modifications\n",
        "          self.b[i] = tf.Variable(1.0, shape=tf.TensorShape(None)) \n",
        "          if (self.layers[-2] >= self.new_topology[-2]):\n",
        "            neurons_to_act = self.new_topology[i-1]\n",
        "            random_index = np.random.choice(self.layers[self.L-2], neurons_to_act, replace=False)\n",
        "            aux = pd.DataFrame(prev_W[self.L -1].numpy())\n",
        "            aux = aux.reindex(columns=random_index).dropna(axis=1)\n",
        "            self.W[i].assign(aux.to_numpy())\n",
        "          else:\n",
        "            neurons_to_act = self.new_topology[-2] - self.layers[-2]\n",
        "            aux = tf.Variable(tf.random.normal(shape=(self.new_topology[i], neurons_to_act)))\n",
        "            self.W[i].assign(tf.concat(axis=1, values=[prev_W[self.L -1], aux]))\n",
        "\n",
        "          self.b[i].assign(prev_b[self.L -1])           \n",
        "          break\n",
        "      else: # The new topology contains the same layers but might differ in the number of units per layer\n",
        "        self.activate_neurons(i)\n",
        "    \n",
        "    # Update layers attributes and keys\n",
        "    self.L = len(self.new_topology)\n",
        "    self.layers = self.new_topology\n",
        "    # self.update_keys()\n",
        "#-------------------------------------------------------------------------------\n",
        "  def train(self, x_train, y_train, x_test, y_test, epochs, steps_per_epoch, batch_size, lr, trigger):\n",
        "\n",
        "      history = {\n",
        "          'val_loss':[],\n",
        "          'train_loss':[],\n",
        "          'val_acc':[]\n",
        "      }\n",
        "      \n",
        "      flag = 0\n",
        "      for e in range(0, epochs):\n",
        "          epoch_train_loss = 0.\n",
        "          print('Epoch{}'.format(e), end='.')\n",
        "          for i in range(0, steps_per_epoch):\n",
        "              x_batch = x_train[i*batch_size:(i+1)*batch_size]\n",
        "              y_batch = y_train[i*batch_size:(i+1)*batch_size]\n",
        "              \n",
        "              batch_loss = self.train_on_batch(x_batch, y_batch,lr)\n",
        "              epoch_train_loss += batch_loss\n",
        "              \n",
        "              if i%int(steps_per_epoch/10) == 0:\n",
        "                  print(end='.')\n",
        "                  \n",
        "          history['train_loss'].append(epoch_train_loss/steps_per_epoch)\n",
        "          val_A = self.forward_pass(x_test)\n",
        "          val_loss = self.compute_loss(val_A, y_test).numpy()\n",
        "          history['val_loss'].append(val_loss)\n",
        "          val_preds = self.predict(x_test)\n",
        "          val_acc =    np.mean(np.argmax(y_test, axis=1) == val_preds.numpy())\n",
        "          history['val_acc'].append(val_acc)\n",
        "          print('Val acc:',val_acc)\n",
        "          \n",
        "          if e>1 & flag == 0:\n",
        "            flag = 1\n",
        "            if (np.abs(history['train_loss'][-2]-history['train_loss'][-1]/history['train_loss'][-2])>=trigger):\n",
        "              print('AG trigger') \n",
        "              self.new_topology = [28*28, 128, 54, 128, 10]\n",
        "              self.AG_update()\n",
        "      return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS6EdO-Qbx8a"
      },
      "source": [
        "# Topological parameters\n",
        "max_layers = 3+2\n",
        "num_max_units = 128\n",
        "input_dim = 28*28\n",
        "output_dim = 10\n",
        "\n",
        "layers = np.zeros(max_layers, dtype='uint32')\n",
        "for i in range(max_layers): \n",
        "  if i == 0:\n",
        "    layers[i] = input_dim\n",
        "  elif i == max_layers-1:\n",
        "    layers[i] = output_dim\n",
        "  else:\n",
        "    layers[i] = num_max_units"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rHIwDLiY4Sg"
      },
      "source": [
        "net = FluidNetwork(layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRBAcbfIrbO-"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hh1qxdL_Y6d3",
        "outputId": "6f67cea1-4849-4415-b429-f46a17e31e60"
      },
      "source": [
        "# Training parameters\n",
        "batch_size = 120\n",
        "epochs = 3\n",
        "steps_per_epoch = int(x_train.shape[0]/batch_size)\n",
        "lr = 3e-3\n",
        "trigger = 0.1\n",
        "print('Steps per epoch', steps_per_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps per epoch 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GA7SLnCQY8g",
        "outputId": "640b3b34-7c63-4bc8-e4b9-facbc3ae1ca3"
      },
      "source": [
        "layers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([784, 128, 128, 128,  10], dtype=uint32)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsecEbJA4eqR"
      },
      "source": [
        "net = FluidNetwork(layers) # pasa algo cuando tiene mas neuronas la primera capa y el numero es mayor o igual a la estructura anterior"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffTuGq-mvhxG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "aa2b9fe0-1ded-4cdc-ab5a-314e10b156f0"
      },
      "source": [
        "# COMPROBACIONES:\n",
        "net.new_topology = [784, 3, 300, 20, 10]\n",
        "net.AG_update()\n",
        "for i in range(1, net.L):\n",
        " print(net.W[i].numpy().shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-a67519bb35ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# COMPROBACIONES:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_topology\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAG_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m  \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-36682206b278>\u001b[0m in \u001b[0;36mAG_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m           \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# The new topology contains the same layers but might differ in the number of units per layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivate_neurons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;31m# Update layers attributes and keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-36682206b278>\u001b[0m in \u001b[0;36mactivate_neurons\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# W\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_topology\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneurons_to_act\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m       \u001b[0;31m# Then add rows to W[i-1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0;31m# W\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1767\u001b[0m           dtype=dtypes.int32).get_shape().assert_has_rank(0)\n\u001b[1;32m   1768\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1769\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mconcat_v2\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1211\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6939\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6940\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6941\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6942\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: ConcatOp : Dimensions of inputs should match: shape[0] = [128,128] vs. shape[1] = [20,172] [Op:ConcatV2] name: concat"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "essBWHmy9VLb"
      },
      "source": [
        "prev_W = net.W.copy()\n",
        "prev_b = net.b.copy()\n",
        "new_L = len(net.new_topology)\n",
        "\n",
        "for i in range(2, new_L): # 2 as the input alwyas remain the same\n",
        "  if net.L < new_L : # The new topology contains more layers\n",
        "    if i <= net.L - 1 : # Only apply for hidden layers\n",
        "      net.activate_neurons(i)\n",
        "    else: # add new layers,and reestructure output weights for the last layer\n",
        "      if i == new_L - 1: # copy the output weights from the previous structure\n",
        "        net.W[i] = tf.Variable(1.0, shape=tf.TensorShape(None)) # to enable future modifications\n",
        "        net.b[i] = tf.Variable(1.0, shape=tf.TensorShape(None)) \n",
        "        if (net.layers[-2] >= net.new_topology[-2]):\n",
        "          neurons_to_act = net.new_topology[i-1]\n",
        "          random_index = np.random.choice(net.layers[net.L-2], neurons_to_act, replace=False)\n",
        "          aux = pd.DataFrame(prev_W[net.L -1].numpy())\n",
        "          aux = aux.reindex(columns=random_index).dropna(axis=1)\n",
        "          net.W[i].assign(aux.to_numpy())\n",
        "        else:\n",
        "          neurons_to_act = net.new_topology[-2] - net.layers[-2]\n",
        "          aux = tf.Variable(tf.random.normal(shape=(net.new_topology[i], neurons_to_act)))\n",
        "          net.W[i].assign(tf.concat(axis=1, values=[prev_W[net.L -1], aux]))\n",
        "\n",
        "        net.b[i].assign(prev_b[net.L -1])\n",
        "      # Add more layers\n",
        "      net.W[i-1] = tf.Variable(1.0, shape=tf.TensorShape(None)) # to enable future modifications\n",
        "      net.b[i-1] = tf.Variable(1.0, shape=tf.TensorShape(None)) \n",
        "      net.W[i-1].assign(tf.Variable(tf.random.normal(shape=(net.new_topology[i-1],net.new_topology[i-2]))))\n",
        "      net.b[i-1].assign(tf.Variable(tf.random.normal(shape=(net.new_topology[i-1],1))))\n",
        "\n",
        "  elif net.L > new_L: # The new topology contains less layers\n",
        "    # Only apply for hidden layers\n",
        "    net.activate_neurons(i)\n",
        "    if i == new_L - 1 :\n",
        "      # First remove extra layers and then assign the ouput weights to the output layer\n",
        "      index_aux = np.arange(start=1, stop=new_L-1, step=1)\n",
        "      # index_aux[-1] = self.L - 1\n",
        "      mask = np.isin(list(net.W.keys()), index_aux) == False\n",
        "      keys_to_remove = np.array(list(net.W.keys()))[mask]\n",
        "      # W\n",
        "      d = net.W\n",
        "      l = keys_to_remove\n",
        "      list(map(d.__delitem__, filter(d.__contains__,l)))\n",
        "      # b\n",
        "      d = net.b\n",
        "      l = keys_to_remove\n",
        "      list(map(d.__delitem__, filter(d.__contains__,l)))\n",
        "      # Assign output weights to the ouput layer\n",
        "      net.W[i] = tf.Variable(1.0, shape=tf.TensorShape(None)) # to enable future modifications\n",
        "      net.b[i] = tf.Variable(1.0, shape=tf.TensorShape(None)) \n",
        "      if (net.layers[-2] >= net.new_topology[-2]):\n",
        "        neurons_to_act = net.new_topology[i-1]\n",
        "        random_index = np.random.choice(net.layers[net.L-2], neurons_to_act, replace=False)\n",
        "        aux = pd.DataFrame(prev_W[net.L -1].numpy())\n",
        "        aux = aux.reindex(columns=random_index).dropna(axis=1)\n",
        "        net.W[i].assign(aux.to_numpy())\n",
        "      else:\n",
        "        neurons_to_act = net.new_topology[-2] - net.layers[-2]\n",
        "        aux = tf.Variable(tf.random.normal(shape=(net.new_topology[i], neurons_to_act)))\n",
        "        net.W[i].assign(tf.concat(axis=1, values=[prev_W[net.L -1], aux]))\n",
        "\n",
        "      net.b[i].assign(prev_b[net.L -1])           \n",
        "      break\n",
        "  else: # The new topology contains the same layers but might differ in the number of units per layer\n",
        "    net.activate_neurons(i)\n",
        "\n",
        "# Update layers attributes and keys\n",
        "net.L = len(net.new_topology)\n",
        "net.layers = net.new_topology"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "88yJk2u8-SAs",
        "outputId": "0b73210a-d497-42e0-f7d3-015b56c62eb0"
      },
      "source": [
        "\n",
        "if net.layers[i-1] < net.new_topology[i-1]: # activate more neurons\n",
        "  neurons_to_act = net.new_topology[i-1] - net.layers[i-1]\n",
        "  if (i != 2  & net.L < len(net.new_topology)) | net.L >= len(net.new_topology): # Only if is not the first layer of this case\n",
        "    # First add cols to W[i]\n",
        "    # W\n",
        "    aux = tf.Variable(tf.random.normal(shape=(net.new_topology[i], neurons_to_act)))\n",
        "    net.W[i].assign(tf.concat(axis=1, values=[net.W[i], aux]))\n",
        "  # Then add rows to W[i-1]\n",
        "  # W\n",
        "  aux = tf.Variable(tf.random.normal(shape=(neurons_to_act, net.new_topology[i-2])))\n",
        "  net.W[i-1].assign(tf.concat(axis=0, values=[net.W[i-1], aux]))\n",
        "  # b\n",
        "  aux = tf.Variable(tf.random.normal(shape=(neurons_to_act, 1)))\n",
        "  net.b[i-1].assign(tf.concat(axis=0, values=[net.b[i-1], aux]))\n",
        "elif net.layers[i-1] > net.new_topology[i-1]: # deactivate some randomly neurons\n",
        "  neurons_to_act = net.new_topology[i-1]\n",
        "  random_index = np.random.choice(net.layers[i-1], neurons_to_act, replace=False)\n",
        "  if (i != net.L - 1 & net.L < len(net.new_topology)) | net.L >= len(net.new_topology): # Only if is not the last layer\n",
        "    # First remove extra cols from the current layer\n",
        "    # W\n",
        "    aux = pd.DataFrame(net.W[i].numpy())\n",
        "    aux = aux.reindex(columns=random_index).dropna(axis=1)\n",
        "    net.W[i].assign(aux.to_numpy())\n",
        "  # Then remove extra rows from the previous layer\n",
        "  # W\n",
        "  aux = np.transpose(pd.DataFrame(net.W[i-1].numpy()))\n",
        "  aux = aux.reindex(columns=random_index).dropna(axis=1)\n",
        "  net.W[i-1].assign(aux.T.to_numpy())\n",
        "  # b\n",
        "  aux = np.transpose(pd.DataFrame(net.b[i-1].numpy()))\n",
        "  aux = aux.reindex(columns=random_index).dropna(axis=1)\n",
        "  net.b[i-1].assign(aux.T.to_numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-7ba34b660dd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# W\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_topology\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneurons_to_act\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0;31m# Then add rows to W[i-1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# W\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1767\u001b[0m           dtype=dtypes.int32).get_shape().assert_has_rank(0)\n\u001b[1;32m   1768\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1769\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mconcat_v2\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1211\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6939\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6940\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6941\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6942\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: ConcatOp : Dimensions of inputs should match: shape[0] = [30,10] vs. shape[1] = [129,190] [Op:ConcatV2] name: concat"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h57_VepeY7JV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "898b0250-243b-46ec-8a74-0a5e96fd99a4"
      },
      "source": [
        "history = net.train(\n",
        "    x_train,y_train,\n",
        "    x_test, y_test,\n",
        "    epochs, steps_per_epoch,\n",
        "    batch_size, lr, trigger)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch0...........Val acc: 0.2528\n",
            "Epoch1...........Val acc: 0.2592\n",
            "AG trigger\n",
            "Epoch2...........Val acc: 0.2083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "DSJDNuPHWjuV",
        "outputId": "473e0f29-9276-4151-d2be-405edc0cb235"
      },
      "source": [
        "plot_results(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<module 'matplotlib.pyplot' from '/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py'>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWMWho2YzTL2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cdA-Z8pzTxO"
      },
      "source": [
        "### Opcion con padding en los pesos (SOL)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2etl9xFtzTxO"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from operator import itemgetter\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.model_selection import ShuffleSplit"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SR_uW-DtzTxP"
      },
      "source": [
        "X , (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpvjmGxpzTxP"
      },
      "source": [
        "def load_data():\n",
        "  \"Loads data and each time the function is called a new partition of xtrain is used\"\n",
        "  (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "  # Shuffle data\n",
        "  # rs = ShuffleSplit(n_splits=5, test_size=.1)\n",
        "  # x_train, y_train  = rs.get_n_splits(X) pensar \n",
        "  # Normalize and transform to categorical\n",
        "  x_train = np.reshape(x_train, (x_train.shape[0], 784))/255.\n",
        "  x_test = np.reshape(x_test, (x_test.shape[0], 784))/255.\n",
        "  y_train = tf.keras.utils.to_categorical(y_train)\n",
        "  y_test = tf.keras.utils.to_categorical(y_test)\n",
        "\n",
        "  return (x_train, y_train), (x_test, y_test)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRg6Gax1zTxP"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = load_data()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npkv3zdIzTxP"
      },
      "source": [
        "def plot_results(history):\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    epochs = len(history['val_loss'])\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(range(epochs), history['val_loss'], label='Val Loss')\n",
        "    plt.plot(range(epochs), history['train_loss'], label='Train Loss')\n",
        "    plt.xticks(list(range(epochs)))\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(range(epochs), history['val_acc'], label='Val Acc')\n",
        "    plt.xticks(list(range(epochs)))\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    return plt"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3HUyWzFzTxP"
      },
      "source": [
        "class FluidNetwork:\n",
        "#-------------------------------------------------------------------------------    \n",
        "  def __init__(self, layers):\n",
        "      self.layers = layers\n",
        "      self.L = len(layers) # input, hidden & output layer\n",
        "      self.num_features = layers[0]\n",
        "      self.num_classes = layers[-1]\n",
        "      \n",
        "      self.W = {}\n",
        "      self.b = {}\n",
        "      \n",
        "      self.dW = {}\n",
        "      self.db = {}\n",
        "      \n",
        "      self.setup()\n",
        "      self.new_topology = []\n",
        "#-------------------------------------------------------------------------------\n",
        "  def setup(self):\n",
        "      \n",
        "      for i in range(1, self.L):\n",
        "        self.W[i] = tf.Variable(1.0, shape=tf.TensorShape(None)) # to enable future modifications\n",
        "        self.b[i] = tf.Variable(1.0, shape=tf.TensorShape(None)) \n",
        "        self.W[i].assign(tf.Variable(tf.random.normal(shape=(self.layers[i],self.layers[i-1]))))\n",
        "        self.b[i].assign(tf.Variable(tf.random.normal(shape=(self.layers[i],1))))\n",
        "#-------------------------------------------------------------------------------\n",
        "  def forward_pass(self, X):\n",
        "\n",
        "      A = tf.convert_to_tensor(X, dtype=tf.float32)\n",
        "      for i in range(1, self.L):\n",
        "          Z = tf.matmul(A,tf.transpose(self.W[i])) + tf.transpose(self.b[i])\n",
        "          if i != self.L-1:\n",
        "              A = tf.nn.relu(Z)\n",
        "          else:\n",
        "              A = Z\n",
        "      return A\n",
        "#-------------------------------------------------------------------------------\n",
        "  def compute_loss(self, A, Y):\n",
        "      loss = tf.nn.softmax_cross_entropy_with_logits(Y,A)\n",
        "      return tf.reduce_mean(loss)\n",
        "#-------------------------------------------------------------------------------   \n",
        "  def update_params(self, lr):\n",
        "      for i in range(1,self.L):\n",
        "          self.W[i].assign_sub(lr * self.dW[i])\n",
        "          self.b[i].assign_sub(lr * self.db[i])\n",
        "#-------------------------------------------------------------------------------          \n",
        "  def predict(self, X):\n",
        "\n",
        "      A = self.forward_pass(X)\n",
        "      return tf.argmax(tf.nn.softmax(A), axis=1)\n",
        "#-------------------------------------------------------------------------------  \n",
        "  def info(self):\n",
        "      num_params = 0\n",
        "      for i in range(1, self.L):\n",
        "          num_params += self.W[i].shape[0] * self.W[i].shape[1]\n",
        "          num_params += self.b[i].shape[0]\n",
        "      print('Input Features:', self.num_features)\n",
        "      print('Number of Classes:', self.num_classes)\n",
        "      print('Hidden Layers:')\n",
        "      print('--------------')\n",
        "      for i in range(1, self.L-1):\n",
        "          print('Layer {}, Units {}'.format(i, self.layers[i]))\n",
        "      print('--------------')\n",
        "      print('Number of parameters:', num_params)\n",
        "#-------------------------------------------------------------------------------\n",
        "  def train_on_batch(self, X, Y, lr):\n",
        "        \n",
        "      X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
        "      Y = tf.convert_to_tensor(Y, dtype=tf.float32)\n",
        "        \n",
        "      with tf.GradientTape(persistent=True) as tape:\n",
        "          A = self.forward_pass(X)\n",
        "          loss = self.compute_loss(A, Y)\n",
        "      for i in range(1, self.L):\n",
        "          self.dW[i] = tape.gradient(loss, self.W[i])\n",
        "          self.db[i] = tape.gradient(loss, self.b[i])\n",
        "      del tape\n",
        "      self.update_params(lr)\n",
        "      return loss.numpy()\n",
        "#-------------------------------------------------------------------------------    \n",
        "  def update_keys(self):\n",
        "    ini_list = {}\n",
        "    for i in range(1, net.L): ini_list[i] = i\n",
        "    self.b = dict(zip(ini_list, list(self.b.values())))    \n",
        "    self.W = dict(zip(ini_list, list(self.W.values())))   \n",
        "#-------------------------------------------------------------------------------\n",
        "  def activate_neurons(self, i):\n",
        "    \"Activate one neuron means adding a extra cols for W[i] and extra rows for W[i-1]\"\n",
        "    if self.layers[i-1] < self.new_topology[i-1]: # activate more neurons\n",
        "      neurons_to_act = self.new_topology[i-1] - self.layers[i-1]\n",
        "      # First add cols to W[i]\n",
        "      # W\n",
        "      aux = tf.Variable(tf.random.normal(shape=(self.layers[i], neurons_to_act)))\n",
        "      self.W[i].assign(tf.concat(axis=1, values=[self.W[i], aux]))\n",
        "      # Then add rows to W[i-1]\n",
        "      # W\n",
        "      aux = tf.Variable(tf.random.normal(shape=(neurons_to_act, self.new_topology[i-2])))\n",
        "      self.W[i-1].assign(tf.concat(axis=0, values=[self.W[i-1], aux]))\n",
        "      # b\n",
        "      aux = tf.Variable(tf.random.normal(shape=(neurons_to_act, 1)))\n",
        "      self.b[i-1].assign(tf.concat(axis=0, values=[self.b[i-1], aux]))\n",
        "\n",
        "    elif self.layers[i-1] > self.new_topology[i-1]: # deactivate some randomly neurons\n",
        "      neurons_to_act = self.new_topology[i-1]\n",
        "      random_index = np.random.choice(self.layers[i-1], neurons_to_act, replace=False)\n",
        "      # First remove extra cols from the current layer\n",
        "      # W\n",
        "      aux = pd.DataFrame(self.W[i].numpy())\n",
        "      aux = aux.reindex(columns=random_index).dropna(axis=1)\n",
        "      self.W[i].assign(aux.to_numpy())\n",
        "      # Then remove extra rows from the previous layer\n",
        "      # W\n",
        "      aux = np.transpose(pd.DataFrame(self.W[i-1].numpy()))\n",
        "      aux = aux.reindex(columns=random_index).dropna(axis=1)\n",
        "      self.W[i-1].assign(aux.T.to_numpy())\n",
        "      # b\n",
        "      aux = np.transpose(pd.DataFrame(self.b[i-1].numpy()))\n",
        "      aux = aux.reindex(columns=random_index).dropna(axis=1)\n",
        "      self.b[i-1].assign(aux.T.to_numpy())\n",
        "#-------------------------------------------------------------------------------    \n",
        "  def AG_update(self): # weight padding # i is the index for weights not for the layers \n",
        "    prev_W = self.W.copy()\n",
        "    prev_b = self.b.copy()\n",
        "    new_L = len(self.new_topology)\n",
        "\n",
        "    for i in range(2, new_L): # 2 as the input alwyas remain the same\n",
        "      if self.L < new_L : # The new topology contains more layers\n",
        "        if i <= self.L - 2 : # Only apply for hidden layers\n",
        "          self.activate_neurons(i)\n",
        "        else: \n",
        "          if i == new_L - 1: # copy the output weights from the previous structure\n",
        "            self.W[i] = tf.Variable(1.0, shape=tf.TensorShape(None)) # to enable future modifications\n",
        "            self.b[i] = tf.Variable(1.0, shape=tf.TensorShape(None)) \n",
        "            if (self.layers[-2] >= self.new_topology[-2]):\n",
        "              neurons_to_act = self.new_topology[i-1]\n",
        "              random_index = np.random.choice(self.layers[self.L-2], neurons_to_act, replace=False)\n",
        "              aux = pd.DataFrame(prev_W[self.L -1].numpy())\n",
        "              aux = aux.reindex(columns=random_index).dropna(axis=1)\n",
        "              self.W[i].assign(aux.to_numpy())\n",
        "            else:\n",
        "              neurons_to_act = self.new_topology[-2] - self.layers[-2]\n",
        "              aux = tf.Variable(tf.random.normal(shape=(self.new_topology[i], neurons_to_act)))\n",
        "              self.W[i].assign(tf.concat(axis=1, values=[prev_W[self.L -1], aux]))\n",
        "\n",
        "            self.b[i].assign(prev_b[self.L -1])\n",
        "\n",
        "          else: # add more layers and reestructure weights for the previous layer\n",
        "          # Reestructure weights for the last layer\n",
        "            if i == self.L -1 :\n",
        "              if self.layers[i-1] < self.new_topology[i-1]: # activate more neurons\n",
        "                neurons_to_act = self.new_topology[i-1] - self.layers[i-1]\n",
        "                # Then add rows to W[i-1]\n",
        "                # W\n",
        "                aux = tf.Variable(tf.random.normal(shape=(neurons_to_act, self.new_topology[i-2])))\n",
        "                self.W[i-1].assign(tf.concat(axis=0, values=[self.W[i-1], aux]))\n",
        "                # b\n",
        "                aux = tf.Variable(tf.random.normal(shape=(neurons_to_act, 1)))\n",
        "                self.b[i-1].assign(tf.concat(axis=0, values=[self.b[i-1], aux]))\n",
        "\n",
        "              elif self.layers[i-1] > self.new_topology[i-1]: # deactivate some randomly neurons\n",
        "                neurons_to_act = self.new_topology[i-1]\n",
        "                random_index = np.random.choice(self.layers[i-1], neurons_to_act, replace=False)\n",
        "                # Then remove extra rows from the previous layer\n",
        "                # W\n",
        "                aux = np.transpose(pd.DataFrame(self.W[i-1].numpy()))\n",
        "                aux = aux.reindex(columns=random_index).dropna(axis=1)\n",
        "                self.W[i-1].assign(aux.T.to_numpy())\n",
        "                # b\n",
        "                aux = np.transpose(pd.DataFrame(self.b[i-1].numpy()))\n",
        "                aux = aux.reindex(columns=random_index).dropna(axis=1)\n",
        "                self.b[i-1].assign(aux.T.to_numpy())\n",
        "\n",
        "          # Add more layers\n",
        "          self.W[i] = tf.Variable(1.0, shape=tf.TensorShape(None)) # to enable future modifications\n",
        "          self.b[i] = tf.Variable(1.0, shape=tf.TensorShape(None)) \n",
        "          self.W[i].assign(tf.Variable(tf.random.normal(shape=(self.new_topology[i],self.new_topology[i-1]))))\n",
        "          self.b[i].assign(tf.Variable(tf.random.normal(shape=(self.new_topology[i],1))))\n",
        "\n",
        "      elif self.L > new_L: # The new topology contains less layers\n",
        "        if i == new_L - 1 :\n",
        "        # Firstly, add/remove weights from the previous layer\n",
        "          if self.layers[i-1] < self.new_topology[i-1]: # activate more neurons\n",
        "            neurons_to_act = self.new_topology[i-1] - self.layers[i-1]\n",
        "            # Then add rows to W[i-1]\n",
        "            # W\n",
        "            aux = tf.Variable(tf.random.normal(shape=(neurons_to_act, self.new_topology[i-2])))\n",
        "            self.W[i-1].assign(tf.concat(axis=0, values=[self.W[i-1], aux]))\n",
        "            # b\n",
        "            aux = tf.Variable(tf.random.normal(shape=(neurons_to_act, 1)))\n",
        "            self.b[i-1].assign(tf.concat(axis=0, values=[self.b[i-1], aux]))\n",
        "\n",
        "          elif self.layers[i-1] > self.new_topology[i-1]: # deactivate some randomly neurons\n",
        "            neurons_to_act = self.new_topology[i-1]\n",
        "            random_index = np.random.choice(self.layers[i-1], neurons_to_act, replace=False)\n",
        "            # Then remove extra rows from the previous layer\n",
        "            # W\n",
        "            aux = np.transpose(pd.DataFrame(self.W[i-1].numpy()))\n",
        "            aux = aux.reindex(columns=random_index).dropna(axis=1)\n",
        "            self.W[i-1].assign(aux.T.to_numpy())\n",
        "            # b\n",
        "            aux = np.transpose(pd.DataFrame(self.b[i-1].numpy()))\n",
        "            aux = aux.reindex(columns=random_index).dropna(axis=1)\n",
        "            self.b[i-1].assign(aux.T.to_numpy())\n",
        "\n",
        "          # Secondly, remove extra layers and then assign the ouput weights to the output layer\n",
        "          index_aux = np.arange(start=1, stop=new_L-1, step=1)\n",
        "          # index_aux[-1] = self.L - 1\n",
        "          mask = np.isin(list(self.W.keys()), index_aux) == False\n",
        "          keys_to_remove = np.array(list(self.W.keys()))[mask]\n",
        "          # W\n",
        "          d = self.W\n",
        "          l = keys_to_remove\n",
        "          list(map(d.__delitem__, filter(d.__contains__,l)))\n",
        "          # b\n",
        "          d = self.b\n",
        "          l = keys_to_remove\n",
        "          list(map(d.__delitem__, filter(d.__contains__,l)))\n",
        "          # Assign output weights to the ouput layer\n",
        "          self.W[i] = tf.Variable(1.0, shape=tf.TensorShape(None)) # to enable future modifications\n",
        "          self.b[i] = tf.Variable(1.0, shape=tf.TensorShape(None)) \n",
        "          if (self.layers[-2] >= self.new_topology[-2]):\n",
        "            neurons_to_act = self.new_topology[i-1]\n",
        "            random_index = np.random.choice(self.layers[self.L-2], neurons_to_act, replace=False)\n",
        "            aux = pd.DataFrame(prev_W[self.L -1].numpy())\n",
        "            aux = aux.reindex(columns=random_index).dropna(axis=1)\n",
        "            self.W[i].assign(aux.to_numpy())\n",
        "          else:\n",
        "            neurons_to_act = self.new_topology[-2] - self.layers[-2]\n",
        "            aux = tf.Variable(tf.random.normal(shape=(self.new_topology[i], neurons_to_act)))\n",
        "            self.W[i].assign(tf.concat(axis=1, values=[prev_W[self.L -1], aux]))\n",
        "\n",
        "          self.b[i].assign(prev_b[self.L -1])           \n",
        "          break\n",
        "        else:\n",
        "        # Only apply for hidden layers\n",
        "          self.activate_neurons(i)\n",
        "      else: # The new topology contains the same layers but might differ in the number of units per layer\n",
        "        self.activate_neurons(i)\n",
        "\n",
        "    # Update layers attributes and keys\n",
        "    self.L = len(self.new_topology)\n",
        "    self.layers = self.new_topology\n",
        "#-------------------------------------------------------------------------------\n",
        "  def train(self, x_train, y_train, x_test, y_test, epochs, steps_per_epoch, batch_size, lr, trigger):\n",
        "\n",
        "      history = {\n",
        "          'val_loss':[],\n",
        "          'train_loss':[],\n",
        "          'val_acc':[]\n",
        "      }\n",
        "      \n",
        "      flag = 0\n",
        "      for e in range(0, epochs):\n",
        "          epoch_train_loss = 0.\n",
        "          print('Epoch{}'.format(e), end='.')\n",
        "          for i in range(0, steps_per_epoch):\n",
        "              x_batch = x_train[i*batch_size:(i+1)*batch_size]\n",
        "              y_batch = y_train[i*batch_size:(i+1)*batch_size]\n",
        "              \n",
        "              batch_loss = self.train_on_batch(x_batch, y_batch,lr)\n",
        "              epoch_train_loss += batch_loss\n",
        "              \n",
        "              if i%int(steps_per_epoch/10) == 0:\n",
        "                  print(end='.')\n",
        "                  \n",
        "          history['train_loss'].append(epoch_train_loss/steps_per_epoch)\n",
        "          val_A = self.forward_pass(x_test)\n",
        "          val_loss = self.compute_loss(val_A, y_test).numpy()\n",
        "          history['val_loss'].append(val_loss)\n",
        "          val_preds = self.predict(x_test)\n",
        "          val_acc =    np.mean(np.argmax(y_test, axis=1) == val_preds.numpy())\n",
        "          history['val_acc'].append(val_acc)\n",
        "          print('Val acc:',val_acc)\n",
        "          \n",
        "          if e>1 & flag == 0:\n",
        "            flag = 1\n",
        "            if (np.abs(history['train_loss'][-2]-history['train_loss'][-1]/history['train_loss'][-2])>=trigger):\n",
        "              print('AG trigger') \n",
        "              self.new_topology = [28*28, 128, 54, 128, 10]\n",
        "              self.AG_update()\n",
        "      return history"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_6h6OOFzTxP"
      },
      "source": [
        "# Topological parameters\n",
        "max_layers = 3+2\n",
        "num_max_units = 128\n",
        "input_dim = 28*28\n",
        "output_dim = 10\n",
        "\n",
        "layers = np.zeros(max_layers, dtype='uint32')\n",
        "for i in range(max_layers): \n",
        "  if i == 0:\n",
        "    layers[i] = input_dim\n",
        "  elif i == max_layers-1:\n",
        "    layers[i] = output_dim\n",
        "  else:\n",
        "    layers[i] = num_max_units"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IA5zF12ZzTxQ"
      },
      "source": [
        "net = FluidNetwork(layers)"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5HmnqkOzTxQ"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = load_data()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PK4QGC6_zTxQ",
        "outputId": "805426b1-b312-47ba-9971-00c48275d90d"
      },
      "source": [
        "# Training parameters\n",
        "batch_size = 120\n",
        "epochs = 3\n",
        "steps_per_epoch = int(x_train.shape[0]/batch_size)\n",
        "lr = 3e-3\n",
        "trigger = 0.1\n",
        "print('Steps per epoch', steps_per_epoch)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps per epoch 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kx1BGE4EzTxQ",
        "outputId": "5ed5ae3c-3552-42b9-869b-6e8f58fcc4d9"
      },
      "source": [
        "# COMPROBACIONES:\n",
        "net = FluidNetwork(layers) \n",
        "net.new_topology = [784,  200, 1, 300, 10]\n",
        "net.AG_update()\n",
        "for i in range(1, net.L):\n",
        " print(net.W[i].numpy().shape)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200, 784)\n",
            "(1, 200)\n",
            "(300, 1)\n",
            "(10, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "262CZiIlzTxR",
        "outputId": "898b0250-243b-46ec-8a74-0a5e96fd99a4"
      },
      "source": [
        "history = net.train(\n",
        "    x_train,y_train,\n",
        "    x_test, y_test,\n",
        "    epochs, steps_per_epoch,\n",
        "    batch_size, lr, trigger)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch0...........Val acc: 0.2528\n",
            "Epoch1...........Val acc: 0.2592\n",
            "AG trigger\n",
            "Epoch2...........Val acc: 0.2083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "Fn2bZ6WUzTxR",
        "outputId": "473e0f29-9276-4151-d2be-405edc0cb235"
      },
      "source": [
        "plot_results(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<module 'matplotlib.pyplot' from '/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py'>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x288 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsgYxvzHCb2J"
      },
      "source": [
        "### GA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GG3KBCtCafP"
      },
      "source": [
        "class GA():\n",
        "  \"Each cromosome represents a possible structure of a new NN\"\n",
        "#-------------------------------------------------------------------------------\n",
        "  def __init__(self, main_fluid_net, num_max_units, num_min_units, num_max_layers, num_min_layers,input_dim, output_dim):\n",
        "    self.num_max_units = num_max_units\n",
        "    self.num_min_units = num_min_units\n",
        "    self.num_max_layers = num_max_layers\n",
        "    self.input_dim = input_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.pop_layers = []\n",
        "    self.n_pop = []\n",
        "    self.best_fitness = []\n",
        "    self.best_NN = []\n",
        "    self.pop_net = [] # the diff among pop_layers and pop_net is that the last one also incluse opt weights\n",
        "    self.pop_fitness = []\n",
        "    self.num_min_layers = num_min_layers\n",
        "    self.pop_initial()\n",
        "    self.main_fluid_net = main_fluid_net\n",
        "#-------------------------------------------------------------------------------    \n",
        "  def pop_initial(self):\n",
        "    # n_pop = int(np.ceil((self.input_dim * 50) / (5 + self.input_dim)))\n",
        "    n_pop = 3\n",
        "    n_layers =  np.random.randint(low=self.num_min_layers, high=self.num_max_layers, size=n_pop)\n",
        "    \n",
        "    pop_layers = [] # Pop of differents NN\n",
        "    for i in range(n_pop):\n",
        "      num_units_layer = np.random.randint(low=self.num_min_units, high=self.num_max_units, size=n_layers[i])\n",
        "      pop_layers.append(num_units_layer)\n",
        "\n",
        "    self.pop_initial = pop_layers\n",
        "    self.n_pop = len(self.pop_initial)\n",
        "    self.pop_layers = pop_layers\n",
        "\n",
        "    # return pop_layers\n",
        "#-------------------------------------------------------------------------------\n",
        "  def fitness(self, epochs):\n",
        "    pop_fitness = []\n",
        "    pop_net = []\n",
        "    pop_layers = []\n",
        "    for i in range(self.n_pop):\n",
        "      layers = self.pop_layers[i]\n",
        "      if layers[0] != self.input_dim:\n",
        "        layers = np.insert(layers, 0, self.input_dim)\n",
        "      if layers[-1] != self.output_dim:\n",
        "        layers = np.insert(layers, len(layers), self.output_dim)\n",
        "      # Create de NN, starting from the previous step\n",
        "      net = self.main_fluid_net\n",
        "      net.new_topology = layers\n",
        "      # Padding\n",
        "      net.AG_update()\n",
        "      # Training parameters\n",
        "      batch_size = 120\n",
        "      steps_per_epoch = int(x_train.shape[0]/batch_size)\n",
        "      lr = 3e-3\n",
        "      trigger = np.inf # to avoid an AG's trigger action\n",
        "      # Training loop\n",
        "      print(f'NN structure {i}/{self.n_pop}')\n",
        "      (x_train, y_train, x_test, y_test) = load_data()\n",
        "      history = net.train(x_train, y_train, x_test, y_test, epochs, steps_per_epoch,\n",
        "        batch_size, lr, trigger)\n",
        "      pop_fitness.append(history['val_acc'][-1])\n",
        "      pop_net.append(net)\n",
        "      pop_layers.append(layers)\n",
        "\n",
        "    self.pop_layers = pop_layers\n",
        "    self.pop_fitness = pop_fitness\n",
        "    self.pop_net = pop_net \n",
        "    return pop_fitness\n",
        "#-------------------------------------------------------------------------------\n",
        "  def selection(self):\n",
        "    pop_fitness =  self.pop_fitness\n",
        "    # Although the fitness values (accuracy) are normalised, as they are compared \n",
        "    # with a random threshold between 0-1 we rather apply a min-max scaler.\n",
        "    fitness_pu = np.asarray(pop_fitness)\n",
        "    fitness_pu = (fitness_pu - fitness_pu.min()) / (fitness_pu.max() - fitness_pu.min()) \n",
        "    index = random.sample(range(len(fitness_pu)), len(fitness_pu))\n",
        "\n",
        "    fathers_to_select = []\n",
        "    j = 0\n",
        "    while (len(fathers_to_select) < 2 and j < len(index)):\n",
        "      i = index[j]\n",
        "      threshold = np.random.random()\n",
        "      if fitness_pu[i] > threshold:\n",
        "        fathers_to_select.append(i)\n",
        "      j +=1\n",
        "    return fathers_to_select\n",
        "#-------------------------------------------------------------------------------\n",
        "  def crossover(self, crossover_rate, nchild): # Two children by each recombination\n",
        "    threshold = np.random.random()\n",
        "    if crossover_rate > threshold: # Then crossover operator is computed\n",
        "      # Select crossover point that is not on the end of the string\n",
        "      c = self.permutation(2) # number of children\n",
        "      # Wrap up all children\n",
        "      # c1, c2 = list(c.values())\n",
        "      return c\n",
        "    else:\n",
        "      return []\n",
        "#-------------------------------------------------------------------------------\n",
        "  def permutation(self, nchild):\n",
        "    c = {}\n",
        "    fathers_to_select = self.selection()\n",
        "    if len(fathers_to_select) < 2:\n",
        "      return []\n",
        "    else:\n",
        "      p1 = self.pop_layers[fathers_to_select[0]]\n",
        "      p2 = self.pop_layers[fathers_to_select[1]]\n",
        "      w_dW1 = np.abs(self.compute_db_weight(fathers_to_select[0]))\n",
        "      w_dW2 = np.abs(self.compute_db_weight(fathers_to_select[1]))\n",
        "      for j in range(2):\n",
        "        c[j] = []\n",
        "        n = np.random.randint(self.num_min_layers, (np.max([len(p1), len(p2)]))) # child's length\n",
        "        for i in range(n):\n",
        "          if i < min([len(p1)-1, len(p2)-1]):\n",
        "            idx = np.random.randint(1, np.min([len(p1)-1, len(p2)-1])) # -1 OJO\n",
        "            aux = np.argmin([w_dW1[idx], w_dW2[idx]])\n",
        "            if aux == 0:\n",
        "              c[j].append(p1[idx])\n",
        "            else:\n",
        "              c[j].append(p2[idx])\n",
        "          else:\n",
        "            if len(p1) > len(p2):\n",
        "              idx = np.random.randint(1, len(p1))\n",
        "              c[j].append(p1[idx])\n",
        "            else:\n",
        "              idx = np.random.randint(1, len(p2))\n",
        "              c[j].append(p2[idx])\n",
        "      return c\n",
        "#-------------------------------------------------------------------------------\n",
        "  def mutation(self, mutation_rate):\n",
        "    threshold = np.random.random()\n",
        "    if threshold < mutation_rate:\n",
        "       n_mut_pop = np.int(self.n_pop*mutation_rate)\n",
        "       return np.random.randint(low=self.num_min_layers, high=self.num_max_layers, size=n_mut_pop)\n",
        "    else:\n",
        "       return []\n",
        "#-------------------------------------------------------------------------------\n",
        "  def generational_replacement(self): \n",
        "    next_pop = []\n",
        "    i = 0\n",
        "    next_pop.append(self.best_NN)\n",
        "    while len(next_pop) < self.n_pop :\n",
        "      aux1 = self.crossover(0.8, 2)\n",
        "      aux2 = self.mutation(0.2)\n",
        "      if len(aux1) != 0:\n",
        "        aux1_0 = list(aux1.values())[0]\n",
        "        aux1_1 = list(aux1.values())[1]\n",
        "        next_pop.append(aux1_0)\n",
        "        next_pop.append(aux1_1)\n",
        "      if len(aux2) != 0:\n",
        "        next_pop.append(list(aux2))\n",
        "      i +=1\n",
        "    self.pop_layers = next_pop\n",
        "#-------------------------------------------------------------------------------\n",
        "  def compute_db_weight(self, j):\n",
        "    mean_dW = []\n",
        "    net = self.pop_net[j]\n",
        "    fitness = self.pop_fitness[j]\n",
        "    for i in range(1, net.L):\n",
        "      mean_dW.append(np.mean(net.dW[i]))\n",
        "    return mean_dW/fitness\n",
        "#-------------------------------------------------------------------------------\n",
        "  def run_algo(self, n_stall_generations, max_iters):\n",
        "     # Parameters' initialization\n",
        "     self.best_fitness = 0\n",
        "     stall_generations = 0\n",
        "     iters = 0\n",
        "     # While-loop to find the optimal parameters\n",
        "     while stall_generations < n_stall_generations or iters < max_iters:\n",
        "       iters += 1\n",
        "       pop_fitness = self.fitness(3)\n",
        "       if self.best_fitness < max(pop_fitness):\n",
        "         self.best_fitness = max(pop_fitness)\n",
        "         max_index = pop_fitness.index(self.best_fitness)\n",
        "         self.best_NN = self.pop_layers[max_index]\n",
        "         stall_generations = 0\n",
        "         print(f'Iter {iters}.The best NN has an accuracy of {self.best_fitness }')\n",
        "       else:\n",
        "         stall_generations += 1\n",
        "         print(f'Iter {iters}.No improvement')\n",
        "\n",
        "       self.generational_replacement()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEKYJcR6X-Jh"
      },
      "source": [
        "num_max_units = 128\n",
        "num_min_units = 10\n",
        "num_max_layers = 5\n",
        "num_min_layers = 2\n",
        "input_dim = 28*28\n",
        "output_dim = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXR3njMZUc9L"
      },
      "source": [
        "algo = GA(net, num_max_units, num_min_units, num_max_layers, num_min_layers, input_dim, output_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gpw4kLacmicT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnJrHBbTmX6X"
      },
      "source": [
        "pop_fitness = []\n",
        "pop_net = []\n",
        "pop_layers = []\n",
        "for i in range(algo.n_pop):\n",
        "  layers = algo.pop_layers[i]\n",
        "  if layers[0] != algo.input_dim:\n",
        "    layers = np.insert(layers, 0, algo.input_dim)\n",
        "  if layers[-1] != algo.output_dim:\n",
        "    layers = np.insert(layers, len(layers), algo.output_dim)\n",
        "  # Create de NN, starting from the previous step\n",
        "  # net = algo.main_fluid_net\n",
        "  net.new_topology = layers\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GEVqmOwnJe6",
        "outputId": "d9942356-8454-4a51-920d-162b1bab6c2b"
      },
      "source": [
        "net.layers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([784, 128, 128, 128,  10], dtype=uint32)"
            ]
          },
          "metadata": {},
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ui3cP_How_B",
        "outputId": "6d473542-0c66-4ac8-885d-440ae783be66"
      },
      "source": [
        "net.AG_update"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method FluidNetwork.AG_update of <__main__.FluidNetwork object at 0x7f8490e56f10>>"
            ]
          },
          "metadata": {},
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEnV4FoHnNxM",
        "outputId": "ba1a04f7-11ad-453f-ba8f-f09b5122b702"
      },
      "source": [
        "a = net.new_topology\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([784,  33,  11,  99,  95,  10])"
            ]
          },
          "metadata": {},
          "execution_count": 237
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJvVIh1Ons-S",
        "outputId": "0ce262ca-98bf-4fe6-ccf4-d6fe2087f2bd"
      },
      "source": [
        "for i in range(1, net.L):\n",
        " print(net.W[i].numpy().shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(33, 784)\n",
            "(11, 128)\n",
            "(99, 128)\n",
            "(95, 99)\n",
            "(10, 95)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9GvFojprbJY",
        "outputId": "769c66b1-463e-43f0-c764-cdae8a8d8025"
      },
      "source": [
        "net.new_topology = [784, 128, 10, 10]\n",
        "net.AG_update\n",
        "for i in range(1, net.L):\n",
        " print(net.W[i].numpy().shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 784)\n",
            "(128, 128)\n",
            "(128, 128)\n",
            "(10, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "1hdI0V8vnGvb",
        "outputId": "e1f983d4-66ef-4bb6-ea25-a1cc908ad6c4"
      },
      "source": [
        "    # Padding\n",
        "    net.AG_update()\n",
        "    # Training parameters\n",
        "    batch_size = 120\n",
        "    steps_per_epoch = int(x_train.shape[0]/batch_size)\n",
        "    lr = 3e-3\n",
        "    trigger = np.inf # to avoid an AG's trigger action\n",
        "    # Training loop\n",
        "    print(f'NN structure {i}/{algo.n_pop}')\n",
        "    history = net.train(x_train, y_train, x_test, y_test, 1, steps_per_epoch,\n",
        "      batch_size, lr, trigger)\n",
        "    pop_fitness.append(history['val_acc'][-1])\n",
        "    pop_net.append(net)\n",
        "    pop_layers.append(layers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN structure 2/3\n",
            "Epoch0."
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-228-808f7f6b3274>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'NN structure {i}/{algo.n_pop}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m history = net.train(x_train, y_train, x_test, y_test, 1, steps_per_epoch,\n\u001b[0;32m---> 11\u001b[0;31m   batch_size, lr, trigger)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mpop_fitness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mpop_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-7e4b78155ab1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_train, y_train, x_test, y_test, epochs, steps_per_epoch, batch_size, lr, trigger)\u001b[0m\n\u001b[1;32m    207\u001b[0m               \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m               \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m               \u001b[0mepoch_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-7e4b78155ab1>\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, X, Y, lr)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersistent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m           \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-7e4b78155ab1>\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m           \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m               \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, name)\u001b[0m\n\u001b[1;32m   3653\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m         return gen_math_ops.mat_mul(\n\u001b[0;32m-> 3655\u001b[0;31m             a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   3656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5694\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5695\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5696\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5697\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5698\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6939\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6940\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6941\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6942\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: In[0] mismatch In[1] shape: 33 vs. 128: [120,33] [128,11] 0 0 [Op:MatMul]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zB4IO6Dn7KZ",
        "outputId": "c3f4588c-b6d9-424c-a86e-c74dfddd9dd5"
      },
      "source": [
        "prev_W = net.W.copy()\n",
        "prev_b = net.b.copy()\n",
        "new_L = len(net.new_topology)\n",
        "\n",
        "for i in range(2, new_L): # 2 as the input alwyas remain the same\n",
        "  if net.L < new_L : # The new topology contains more layers\n",
        "    if i <= net.L - 1 : # Only apply for hidden layers\n",
        "      net.activate_neurons(i)\n",
        "    else: # add new layers,and reestructure output weights for the last layer\n",
        "      if i == new_L - 1: # copy the output weights from the previous structure\n",
        "        self.W[i] = tf.Variable(1.0, shape=tf.TensorShape(None)) # to enable future modifications\n",
        "        self.b[i] = tf.Variable(1.0, shape=tf.TensorShape(None)) \n",
        "        if (self.layers[-2] >= self.new_topology[-2]):\n",
        "          neurons_to_act = self.new_topology[i-1]\n",
        "          random_index = np.random.choice(self.layers[self.L-2], neurons_to_act, replace=False)\n",
        "          aux = pd.DataFrame(prev_W[self.L -1].numpy())\n",
        "          aux = aux.reindex(columns=random_index).dropna(axis=1)\n",
        "          self.W[i].assign(aux.to_numpy())\n",
        "        else:\n",
        "          neurons_to_act = self.new_topology[-2] - self.layers[-2]\n",
        "          aux = tf.Variable(tf.random.normal(shape=(self.new_topology[i], neurons_to_act)))\n",
        "          self.W[i].assign(tf.concat(axis=1, values=[prev_W[self.L -1], aux]))\n",
        "\n",
        "        self.b[i].assign(prev_b[self.L -1])\n",
        "      # Add more layers\n",
        "      self.W[i-1] = tf.Variable(1.0, shape=tf.TensorShape(None)) # to enable future modifications\n",
        "      self.b[i-1] = tf.Variable(1.0, shape=tf.TensorShape(None)) \n",
        "      self.W[i-1].assign(tf.Variable(tf.random.normal(shape=(self.new_topology[i-1],self.new_topology[i-2]))))\n",
        "      self.b[i-1].assign(tf.Variable(tf.random.normal(shape=(self.new_topology[i-1],1))))\n",
        "\n",
        "  elif self.L > new_L: # The new topology contains less layers\n",
        "    # Only apply for hidden layers\n",
        "    self.activate_neurons(i)\n",
        "    if i == new_L - 1 :\n",
        "      # First remove extra layers and then assign the ouput weights to the output layer\n",
        "      index_aux = np.arange(start=1, stop=new_L-1, step=1)\n",
        "      # index_aux[-1] = self.L - 1\n",
        "      mask = np.isin(list(self.W.keys()), index_aux) == False\n",
        "      keys_to_remove = np.array(list(self.W.keys()))[mask]\n",
        "      # W\n",
        "      d = self.W\n",
        "      l = keys_to_remove\n",
        "      list(map(d.__delitem__, filter(d.__contains__,l)))\n",
        "      # b\n",
        "      d = self.b\n",
        "      l = keys_to_remove\n",
        "      list(map(d.__delitem__, filter(d.__contains__,l)))\n",
        "      # Assign output weights to the ouput layer\n",
        "      self.W[i] = tf.Variable(1.0, shape=tf.TensorShape(None)) # to enable future modifications\n",
        "      self.b[i] = tf.Variable(1.0, shape=tf.TensorShape(None)) \n",
        "      if (self.layers[-2] >= self.new_topology[-2]):\n",
        "        neurons_to_act = self.new_topology[i-1]\n",
        "        random_index = np.random.choice(self.layers[self.L-2], neurons_to_act, replace=False)\n",
        "        aux = pd.DataFrame(prev_W[self.L -1].numpy())\n",
        "        aux = aux.reindex(columns=random_index).dropna(axis=1)\n",
        "        self.W[i].assign(aux.to_numpy())\n",
        "      else:\n",
        "        neurons_to_act = self.new_topology[-2] - self.layers[-2]\n",
        "        aux = tf.Variable(tf.random.normal(shape=(self.new_topology[i], neurons_to_act)))\n",
        "        self.W[i].assign(tf.concat(axis=1, values=[prev_W[self.L -1], aux]))\n",
        "\n",
        "      self.b[i].assign(prev_b[self.L -1])           \n",
        "      break\n",
        "  else: # The new topology contains the same layers but might differ in the number of units per layer\n",
        "    self.activate_neurons(i)\n",
        "\n",
        "# Update layers attributes and keys\n",
        "self.L = len(self.new_topology)\n",
        "self.layers = self.new_topology"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(33, 784)\n",
            "(11, 128)\n",
            "(99, 128)\n",
            "(95, 99)\n",
            "(10, 95)\n"
          ]
        }
      ]
    }
  ]
}